python3 generate_samples.py \
    --model-parallel-size 1 \
    --num-layers 12 \
    --hidden-size 1024 \
    --num-attention-heads 16 \
    --max-position-embeddings 1024 \
    --seq-length 128 \
    --load "checkpoints/gpt2_180m_百科" \
    --tokenizer-type SentencePieceTokenizer \
    --tokenizer-path data/spm/gpt2_huamei_corpus_bpe_32k_v2.model \
    --fp16 \
    --cache-dir cache \
    --out-seq-length 1024 \
    --top_p 1 \