{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文段句格式语料转wikipedia格式jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始预料的格式是 JSON Lines，每一行是一篇文章，其内容是二维数组，第一个维度是段落，第二个维度是句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 代码准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from contextlib import ExitStack\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from glob import glob, iglob\n",
    "from itertools import chain, cycle, islice\n",
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 1024\n",
    "MIN_CTX_LEN = 128\n",
    "\n",
    "SPM_MODEL = '../data/spm/gpt2_huamei_corpus_bpe_32k_v2.model'\n",
    "\n",
    "\n",
    "SP = spm.SentencePieceProcessor()\n",
    "SP.load(SPM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_files_line_iterator(paths):\n",
    "    return chain.from_iterable(\n",
    "        open(path)\n",
    "        for path\n",
    "        in tqdm(paths, '[files     ]', unit='file')\n",
    "    )\n",
    "\n",
    "\n",
    "def single_text_file_line_count(path, show_progress_bar=False):\n",
    "    with open(path) as fd:\n",
    "        iterable = tqdm(fd) if show_progress_bar else fd\n",
    "        return sum(1 for _ in iterable)\n",
    "\n",
    "\n",
    "def text_files_line_count(paths):\n",
    "    try:\n",
    "        total = len(paths)\n",
    "    except (AttributeError, TypeError):\n",
    "        total = None\n",
    "    with Pool() as pool:\n",
    "        it = pool.imap_unordered(\n",
    "            single_text_file_line_count,\n",
    "            tqdm(paths, '[files     ]', unit='file')\n",
    "        )\n",
    "        return sum(c for c in tqdm(it, '[files     ]', unit='file', total=total))\n",
    "\n",
    "\n",
    "def proc_line(line):\n",
    "    result = []\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return result\n",
    "    paragraphs = json.loads(line)\n",
    "    text = ''\n",
    "    n_text = 0\n",
    "    for sentence in chain.from_iterable(paragraphs):\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        n_sentence = len(SP.encode_as_ids(sentence))\n",
    "        if n_text + n_sentence < SEQ_LENGTH + MIN_CTX_LEN // 2:\n",
    "            text += sentence\n",
    "            n_text += n_sentence\n",
    "        else:\n",
    "            result.append({'text': text, 'length': n_text})\n",
    "            text = sentence\n",
    "            n_text = n_sentence\n",
    "    if n_text:\n",
    "        result.append({'text': text, 'length': n_text})\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILES = [\n",
    "    path\n",
    "    for path in tqdm(iglob(\n",
    "        \"/nfs/server01_public/data/gpt2/output/gpt2_huamei_corpus.json.8g/**/*.*\",\n",
    "        recursive=True\n",
    "    ))\n",
    "    if os.path.isfile(path) and os.path.splitext(path)[1].lower() in ('.jsonl', '.jsonlines', 'json', 'jsonline')\n",
    "]\n",
    "print(f'源语料文件数：{len(INPUT_FILES):,d}')\n",
    "\n",
    "total_lines = text_files_line_count(INPUT_FILES)\n",
    "print(f'源语料行数：{total_lines:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = '../data/gpt2_huamei_corpus_10g-1k.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_iterator = text_files_line_iterator(INPUT_FILES)\n",
    "n_samples = 0\n",
    "n_discard = 0\n",
    "\n",
    "with Pool() as pool, \\\n",
    "     open(OUTPUT_FILE, 'w') as fp:\n",
    "    it = pool.imap_unordered(\n",
    "        proc_line,\n",
    "        tqdm(lines_iterator, 'process', total=total_lines),\n",
    "        chunksize=512\n",
    "    )\n",
    "    for result in tqdm(it, 'save', total=total_lines):\n",
    "        for d in result:\n",
    "            if d['length'] < MIN_CTX_LEN:\n",
    "                n_discard += 1\n",
    "                continue\n",
    "            s = json.dumps(d, ensure_ascii=False)\n",
    "            n_samples += 1\n",
    "            print(s, file=fp)\n",
    "\n",
    "print(f'得到语料样本数：{n_samples:,d}')\n",
    "print(f'抛弃语料样本数：{n_discard:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看输出文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = !wc -l {OUTPUT_FILE}\n",
    "n_samples = int(x[0].split()[0])\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "!du -h {OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 长度统计\n",
    "\n",
    "见：  <http://holoviews.org/user_guide/Large_Data.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # noqa\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "from holoviews.operation.datashader import datashade, shade, spread, dynspread, rasterize, spread\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "def iter_corpus_length():\n",
    "    with open(OUTPUT_FILE) as fp:\n",
    "        for line in tqdm(fp, total=n_samples):\n",
    "            data = json.loads(line)\n",
    "            yield {'length': data['length']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iter_corpus_length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = hv.Points(df, ['index', 'length'])\n",
    "spread(datashade(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron-LM]",
   "language": "python",
   "name": "conda-env-Megatron-LM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
