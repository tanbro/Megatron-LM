{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron XinLi QA (婆媳) 预测 (BPE v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD\n",
    "\n",
    "定位到工作目录，根据具体情况决定哦，不一定是下面的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "准备运行这个笔记本的 Jupyter kernel(**如果已经准备就绪，不要重复执行！**)：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 配置一个 Conda 环境作为 Jupyter Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda env update -f environments/environment-ipy.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装完毕后，为该 Notebook 选择这个 Kernel (名为`Megatron_LM-ipy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在Kernel所在 Conda 环境中安装 Apex\n",
    "\n",
    "需要通过 pip 从 github 下载源代码安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -v -r requirements/apex.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 Checkpoints\n",
    "\n",
    "文件比较大，根据实际情况选择下载，**不要重复下载**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "S3_BUCKET = 'huamei'\n",
    "CKPTS_DIR = 'checkpoints/345m-mildil'\n",
    "\n",
    "S3_CKPTS_DIR = 's3://' + os.path.join(S3_BUCKET, CKPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint iter_230000\n",
      "sync: s3://huamei/checkpoints/345m-hmwebmix-bpe-v2/iter_230000 -> checkpoints/345m-hmwebmix-bpe-v2/iter_230000\n",
      "CPU times: user 27.4 ms, sys: 21.1 ms, total: 48.5 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 复制 latest_checkpointed_iteration.txt\n",
    "!aws s3 cp \\\n",
    "    {S3_CKPTS_DIR} \\\n",
    "    {CKPTS_DIR} \\\n",
    "    --recursive \\\n",
    "    --exclude \"*\" \\\n",
    "    --include \"latest_checkpointed_iteration.txt\"\n",
    "\n",
    "# 下载后读取最新的 checkpoint iter 名称\n",
    "iter_step = open(f'{CKPTS_DIR}/latest_checkpointed_iteration.txt').read().strip()\n",
    "ckpt_dir = f'iter_{iter_step}'\n",
    "\n",
    "print(f'checkpoint {ckpt_dir}')\n",
    "\n",
    "s3_ckpt_dir = os.path.join(S3_CKPTS_DIR, ckpt_dir)\n",
    "local_ckpt_dir = os.path.join(CKPTS_DIR, ckpt_dir)\n",
    "\n",
    "print(f'sync: {s3_ckpt_dir} -> {local_ckpt_dir}')\n",
    "    \n",
    "# 同步最新的 Checkpiont\n",
    "!aws s3 sync \\\n",
    "    s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/iter_0230000 \\\n",
    "    ./checkpoints/345m-hmwebmix-bpe-v2/iter_0230000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用哪个/些 GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "from itertools import chain, compress\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from pretrain_gpt2 import get_masks_and_position_ids\n",
    "from predict_gpt2 import initialize_distributed, prepare_tokenizer, set_random_seed, setup_model, get_token_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    num_layers=24,\n",
    "    hidden_size=1024,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=1024,\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=1024,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "    tokenizer_type='GPT2BPETokenizer_CN',\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    cache_dir=None,\n",
    "    # Training arguments.\n",
    "    load='./checkpoints/345m-mildil/',\n",
    "    seed=1234,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    resume_dataloader=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None, \n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "#     out_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化函数/全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def initialize():\n",
    "    global model, tokenizer\n",
    "\n",
    "    # Disable CuDNN.\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Pytorch distributed.\n",
    "    initialize_distributed(args)\n",
    "\n",
    "    # Random seeds for reproducability.\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    # get the tokenizer\n",
    "    tokenizer = prepare_tokenizer(args)\n",
    "\n",
    "    # Model, optimizer, and learning rate.\n",
    "    model = setup_model(args)\n",
    "\n",
    "    args.device = torch.cuda.current_device()\n",
    "\n",
    "    # setting default batch size to 1\n",
    "    args.batch_size = 1\n",
    "\n",
    "    assert mpu.get_model_parallel_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主进程初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "prepare tokenizer done\n",
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 0: 336128000\n",
      "global rank 0 is loading checkpoint ./checkpoints/345m-mildil/iter_0100000/mp_rank_00/model_optim_rng.pt\n",
      "  successfully loaded ./checkpoints/345m-mildil/iter_0100000/mp_rank_00/model_optim_rng.pt\n",
      "CPU times: user 11.7 s, sys: 3.65 s, total: 15.3 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tokens_generative(context_tokens, model, tokenizer):\n",
    "    context_length = len(context_tokens)\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)   \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        yield ids[-1]\n",
    "\n",
    "\n",
    "def infer_text_generative(contex_text, model, tokenizer):\n",
    "    contex_text = contex_text.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(contex_text).tokenization\n",
    "    context_length = len(context_tokens)\n",
    "\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "    \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        s = tokenizer.DecodeIds([ids[-1]])\n",
    "        yield s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证是否可运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    (\n",
    "        '刚看了一些帖子，说在怀孕的时候婆婆做的饭不好吃，或者照顾的不到位，然后媳妇都很委屈，我真的想说，你们这些婆婆在我看来都是天使，什么叫JP的婆家（不是某一个人，而是一家）看看我的遭遇就知道了。我婆婆在我怀孕初期反应剧烈的时候说她没空（其实除了伺候她女儿一家，就是在家看肥皂剧、明星绯闻什么的，这些她样样精通）不能过来照顾我，因为我当时非常严重一个月瘦了十几斤，贫血贫的连床都下不了，在我老公的登门请求下，她三个月内总共给我们弄过2次菜，一次是毒蘑菇，吃的我夜里一点吐血进医院，另一次是死鱼，吃完以后我和老公都拉肚子一个星期，这可都是我怀孕期间她干的事啊，医生严重警告了孕妇前三个月拉肚子很可能导致流产。我老公找她问她为什么要买这些坏掉的菜给孕妇吃，为什么连孙子都不管不顾，她还气的要命呢，说，早说了我没空，我哪有功夫管你们，你们想吃好的自己买去吧。对了，这两样菜是我老公给了1000块钱给他妈才买来的呢。本来是觉得不放心保姆一个人在家照顾我才百般央求婆婆能念在亲情的份上稍微照顾我一点，可惜发现她真的比一个见死不救的路人还要冷血。',\n",
    "        '介绍一下我的婆家，公公婆婆都退休在家，收入可靠，颇有积蓄，但是从我们买房、装修到结婚前后的诸多事件中，他们从来不花一分钱，没出过一点力，任何礼节、规矩，他们全部都假装不知道，从定亲到结婚，所有要用到的大小花销，大到订婚的礼钱，小到连结婚用到的一个囍字、一颗糖，全部都是我老公拿钱出来，并且自己亲自买好的。为了忙结婚我老公瘦了十几斤，但他爸妈从来不闻不问，那他们每天在家干什么呢？我的公公每天一大早就出去打牌，一直打到晚上吃完宵夜才回家。婆婆忙着伺候她年近40身体健康的女儿全家，还要忙着追电视剧，研究明星绯闻。我们装修时进材料请求公公来帮忙看一天，他11点终于到了，11点半就打电话给我老公让他快过来，我老公不知道发生了什么事赶快回家，结果是问我们中午安排他在哪家饭店吃饭，结果那天中午他一个人喝掉一斤白酒，然后在饭店就说：“累了，我回家睡觉了”然后不管不顾走了，感情我们花了300多块钱请他来专门吃饭的，后来没办法，我只好请了半天假在家看着。最让人心寒的是结婚前特别忙的一段时间，因为我老公每天回家都很晚，公婆居然让我老公搬出去住，说是影响他们休息了。蘑菇整个伞面里面全部都长满了一厘米左右长的黑霉，够可怕的！',\n",
    "    ),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刚看了一些帖子，说在怀孕的时候婆婆做的饭不好吃，或者照顾的不到位，然后媳妇都很委屈，我真的想说，你们这些婆婆在我看来都是天使，什么叫JP的婆家（不是某一个人，而是一家）看看我的遭遇就知道了。我婆婆在我怀孕初期反应剧烈的时候说她没空（其实除了伺候她女儿一家，就是在家看肥皂剧、明星绯闻什么的，这些她样样精通）不能过来照顾我，因为我当时非常严重一个月瘦了十几斤，贫血贫的连床都下不了，在我老公的登门请求下，她三个月内总共给我们弄过2次菜，一次是毒蘑菇，吃的我夜里一点吐血进医院，另一次是死鱼，吃完以后我和老公都拉肚子一个星期，这可都是我怀孕期间她干的事啊，医生严重警告了孕妇前三个月拉肚子很可能导致流产。我老公找她问她为什么要买这些坏掉的菜给孕妇吃，为什么连孙子都不管不顾，她还气的要命呢，说，早说了我没空，我哪有功夫管你们，你们想吃好的自己买去吧。对了，这两样菜是我老公给了1000块钱给他妈才买来的呢。本来是觉得不放心保姆一个人在家照顾我才百般央求婆婆能念在亲情的份上稍微照顾我一点，可惜发现她真的比一个见死不救的路人还要冷血。<sep>介绍一下我的婆家，公公婆婆都退休在家，收入可靠，颇有积蓄，但是从我们买房、装修到结婚前后的诸多事件中，他们从来不花一分钱，没出过一点力，任何礼节、规矩，他们全部都假装不知道，从定亲到结婚，所有要用到的大小花销，大到订婚的礼钱，小到连结婚用到的一个囍字、一颗糖，全部都是我老公拿钱出来，并且自己亲自买好的。为了忙结婚我老公瘦了十几斤，但他爸妈从来不闻不问，那他们每天在家干什么呢？我的公公每天一大早就出去打牌，一直打到晚上吃完宵夜才回家。婆婆忙着伺候她年近40身体健康的女儿全家，还要忙着追电视剧，研究明星绯闻。我们装修时进材料请求公公来帮忙看一天，他11点终于到了，11点半就打电话给我老公让他快过来，我老公不知道发生了什么事赶快回家，结果是问我们中午安排他在哪家饭店吃饭，结果那天中午他一个人喝掉一斤白酒，然后在饭店就说：“累了，我回家睡觉了”然后不管不顾走了，感情我们花了300多块钱请他来专门吃饭的，后来没办法，我只好请了半天假在家看着。最让人心寒的是结婚前特别忙的一段时间，因为我老公每天回家都很晚，公婆居然让我老公搬出去住，说是影响他们休息了。蘑菇整个伞面里面全部都长满了一厘米左右长的黑霉，够可怕的！<sep><sep><sep><|endoftext|>\n",
      "\n",
      "0: 这世界上居然还有这种不要脸的一辈子,怀孕想起都恶心!楼主最好回娘家啊!最好回娘家啊!\n",
      "\n",
      "1: 婆婆看来指望不上也可以理解,太在乎小肚鸡肠什么的不大好弄,会得癌反复的\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.recompute=True\n",
    "# args.top_p=0.0\n",
    "# args.top_k=0\n",
    "# args.temperature=1.0\n",
    "\n",
    "n_gen = 2\n",
    "\n",
    "for question_title, question_text in input_texts:\n",
    "    text = question_title.strip() + '<sep>' \\\n",
    "        + question_text.strip()  + '<sep>' \\\n",
    "        + '<sep><sep><|endoftext|>'\n",
    "    print(text)\n",
    "    print()\n",
    "    for i in range(n_gen):\n",
    "#         args.temperature=random.gauss(0.95, 0.05)\n",
    "        print(f'{i}: ', end='')\n",
    "        s_pred = ''\n",
    "        for s in infer_text_generative(text, model, tokenizer):\n",
    "            print(s, end='')\n",
    "        print(os.linesep)\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 test 语料，从中随机打断，并预测下文，比较原文与预测结果！\n",
    "\n",
    "随机选 N 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "之所以发这个帖子是因为之前婆婆一家有太多奇葩事迹，比电视剧还狗血，我这个记性又不好，也不是记性不好，就是心大，不太容易把苦啊那些记在心里，而且很容易别人对我一点好我就把之前的不好都忘了，所以这次婆婆来我家住一个月我准备以血日记的形式记录一下。真的，编剧们可以看看，说不定有启发呢。背景：我和老公结婚3年，备孕1年，居住海外，我全职在家，婆婆此次因为自己的私事来呆一个月。现在已经第6天了，所以前6天简单回溯一下第1天：抵达，老公接机，凌晨2点，我做了粥，炒了两个菜，等着婆婆回来吃。第2天：收拾。第3天：带来了很多瓶瓶罐罐，把调味盒子全部更换了，我才是下厨的人，有些盒子好看是好看，可是调味料根本弄不起来啊。这事我一句没有多提，我想她大老远背过来也不容易。第4天：这天是每周我们洗衣服的日子，我和朋友有约，早早出门了，但鉴于太爱“帮忙”的性格，我出门告诉她千万别帮我们洗衣服，她膝盖不好，洗衣筐要扛到洗衣间确实比较重。（这里介绍下我婆婆是属于特别喜欢沉醉于牺牲奉献的一个人，特别喜欢苦情的戏码。你越是让她不做的事她偏偏要做，让你觉得欠她很多，但这些付出都不是必要的。比如她对紫外线过敏，不能晒太阳，我们去逛街，我在国外不好意思撑伞遮阳就随手拿着那些广告纸宣传纸什么的遮一下，我公公以前有腰肌劳损，我从来没见者犯病过，我就说公公帮妈拿一点购物袋吧，我手上已经很多了，她却说“你爸腰不好”然后一个人大包小包穿梭在阳光里。就是有阳光的地方她就举起挂着各种购物袋有宣传纸打的手挡住阳光，然后跑几步到阴凉的地方，而公公在那儿闲庭信步。所以每次我们从超市采购回家，公公都是一个拎着自己的那包东西，一般是零食种子什么的，而我们其他人大包小包，害的跑几趟）我回家，一进车库，车库里面晾着全是内裤。回到家，家里的摆设也随她的要求变换了位置。我有点生气，觉得她对我们的关爱有点太没有界限了，但也觉得她也是好心就没说什么。回家之后她反而提起今天做的事，我说“我回来之后没说话就是因为不想鼓励你太为我们受累了，我们是想让你多休息”她说“我在家我也不可能不动吧，而且有时差，做事反而分散注意力，不那么想睡觉。我也没说什么了。晚上，老公回家，她进房间睡觉之前和我老公说“好无聊啊，赶快把事情办完我回国了”这只是过了一天我平时天天过的日子啊。我呢，很天真的跟老公说了些洗内裤的事，他没什么回应，然后他心理念着他妈刚刚说要想赶快回国是不是有不开心的，马上说”我上去和我妈聊聊，她是不是有话跟我说”其实这时候我挺心寒的，决定不再多说什么的，这也是我为什么觉得用帖子记录的原因对不起有时候写的时候突然回忆道其实还是有点生气，所以感觉什么细节都想说，以后尽量简化。第5天：中午吃饭，我随口一提说我表弟找到个不错的工作，空乘，工资上万了，我婆婆马上开始酸了，说她朋友的老公在航空公司工作都工作几十年了才十几万一年，我解释说工作可能不一样吧。她又说空乘是青春饭，老了怎么办，还是要学技术，说她以前给一个人说去当学徒那个人还不愿意，现在吃到甜头了，我说但他现在也才十几万一年啊，我表弟大专毕业找到这个工作能解决家庭经济负担的燃眉之急很不错了，一家人都觉得挺幸福的现在。她说但是我指导的那个人人家是农村的啊，起点不一样啊，十几万相当不错了。我就提到了她弟弟的儿子，我说他当公务员不也就3千来块钱吗，她马上说可是人家工作稳定啊，我说航空公司也稳定啊。最后我说人家一家人现在觉得幸福就行了结束了话题。我真想抽自己一嘴巴，没事闲扯干嘛，婆媳就是有事说事最好，扯其他话题拉近距离反而容易有争执啊。大家体会到一点了吧，她们总是觉得我家的都不好。以后再慢慢给大家补充她说我父母的。下午又拿着抹布到处擦，把家里东西都移位了，因为“东西摆多了看着乱”“家里可以脏不能乱”EXCUSE ME ?又可以脏了？她有洁癖强迫症，每天打扫家里，她来之前我整整打扫了一周的卫生，非常细微都的打扫，家里每一个小摆件都得拿下来擦，柜子书桌顶啊那些。第6天：大扫除的日子。早上终于说了洗内裤的事，对话如下她说“我前天给你们洗了14条内裤，内裤必须每天洗！”“但是每天洗怎么烘干，在室内晾干又潮又硬还臭的。”她说“有汗又有分泌物，一个星期洗一次好恶心哦（加重）”我说“美国人都这样啊，人家还是丢尽公共洗衣机和所有衣服混着洗呢，我都是手搓，放到干衣机烘干而已”她说“人种不一样，随便找个地方就晾了”“烘干的经过高温消毒更卫生啊，而且这么久了我们也没得病啊”“其他事我可以不管，这个事必须管！后来我想想何必和她争论呢，顺着她吧。车上我就我应该早采纳她的建议的，反正每周洗14条和每天洗2条都是一样的工作量，我每天用烘干机最高温短时间烘干就行，你提醒的好，不然我都没动脑筋想办法解决这个问题。（其实我是非常在乎自己隐私的人，上学那会我从来不允许我爸妈翻我书包，进我房间必须敲门。我总是因为她没有个人隐私的这个界限而很难受，但我知道和她说绝对说不通）然后这一天，我们从早上8点开始打扫卫生，直到下午5点才做完，我5点半又做晚饭，吃完我收拾洗碗。第7天：本来和朋友有约会，但昨天打扫一天有点累想在家里，就是我现在在书房码字的时候。说说早饭时候，老公刚出门，她说生小孩的事，她来几天我多次告诉她我以前给自己压力太大最近想心情放松下来，我让我家人都不要提要小孩的事。她让我尽快回国找她联系的医生看病看中医，我们其实在国外已经看过了，说是没有原因，反正男方家里还是觉得多半是我的问题吧。说我们都不年轻了，像我这个年纪她都生我老公了，她还算晚育呢，说他们也年纪大了，还搬出我老公的奶奶说，说奶奶说到这个事也挺着急的。说我们刚结婚避孕那会简直是失策了。其实我又不是不想怀孕，我做梦都想呢，自己压力都大的不得了，网上看到或者听说什么对怀孕好，不敢再难喝的东西都喝，再听到别人说这些真的挺难受的。真的，有时候我觉得我都不想拖累人家了，离婚了让他去找能生的吧，耽误人家自己也被束缚，过得一点都不快乐。但我这个婚姻是我奶奶促成的，我堂妹离婚我奶奶说哭了好几个晚上，要是我离婚我都不敢想象我奶奶的心情了<sep>我婆婆呢，退休之前在自己的单位当了个一官半职的，所以她希望我像女儿一样亲近她，有的像下属一样尊重她。她是我见过最爱说自己在单位丰功伟绩的人了，每次说起神采飞扬！她经常说以前她单位的小女孩都特别喜欢她，说谁当她媳妇谁好命，这种恭维话当真的人，我也是呵呵哒。你说如果我和她亲近，亲近就容易有时候一句话说错什么的容易得罪她，或者像自己妈妈一样，有时候拌拌嘴但转眼就好了，她能记你的愁就很久的。她希望你和她亲近但又服从与她，尊重她，爱戴他，你们觉得可行吗？\n",
      "\n",
      "我家公婆还好，他们坚持艰苦朴素的美德，为了节约用水每次洗完菜的水，洗完衣服的水（硬把洗衣机的下水管接到桶里）用来冲马桶。我直接表示我是不会这样做的，并且也不会让我儿子这样做，用干净水冲马桶不叫浪费水！我老公也不止一次表示强烈抗议，无果，现在他们干他们的，我们该干啥干啥，我也很无奈\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = './data/mildil/mildil.test.json'\n",
    "\n",
    "with open(input_file) as fp:\n",
    "    for line in fp:\n",
    "        data = json.loads(line)\n",
    "        text = data['text']\n",
    "for x in text.split('<sep><sep><|endoftext|>'):\n",
    "    print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_file=./data/mildil/mildil.test_10-100000-1024.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e27dcc2c8514f6c9fe47cbdbf720b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 数据总数: 1,000\n",
      "Test 采样数: 1,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac646e97bda41f5a68d59d0a305e08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='sample', max=10, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a200310fbc294a84abe32cd639756417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='infer', max=1000, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "b_random = False\n",
    "b_shuffle = False\n",
    "\n",
    "input_file = './data/mildil/mildil.test.json'\n",
    "output_file = f'./data/mildil/mildil.test_{n_samples}-{iter_step}-{args.seq_length}.tsv'\n",
    "\n",
    "print(f'output_file={output_file}')\n",
    "\n",
    "total = sum(1 for _ in tqdm(open(input_file)))\n",
    "print(f'Test 数据总数: {total:,d}')\n",
    "\n",
    "if n_samples > 0:\n",
    "    assert total >= n_samples\n",
    "else:\n",
    "    n_samples = total\n",
    "\n",
    "print(f'Test 采样数: {n_samples:,d}')\n",
    "\n",
    "mask = np.zeros(total, dtype=int)\n",
    "mask[:n_samples] = 1\n",
    "if b_random:\n",
    "    np.random.shuffle(mask)\n",
    "\n",
    "samples = []\n",
    "with open(input_file) as fp:\n",
    "    reader = compress(fp, mask)\n",
    "    for line in tqdm(reader, 'sample', total=N):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        text = json.loads(line)['text']\n",
    "        samples.append(text)\n",
    "\n",
    "if b_shuffle:\n",
    "    random.shuffle(samples)\n",
    "\n",
    "delimiter = '<sep><sep><|endoftext|>'\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    writer = csv.writer(fp, delimiter='\\t')\n",
    "    writer.writerow(['标题', '内容', '回答', '生成文本'])\n",
    "    for text in tqdm(samples, 'infer'):\n",
    "        question, answer = text.split(delimiter)\n",
    "        question_title, question_text = question.split('<sep>')\n",
    "        input_txt = question + delimiter\n",
    "        infer_txt = ''.join(id_ for id_ in infer_text_generative(input_txt, model, tokenizer))\n",
    "        row = [question_title, question_text, answer, infer_txt]\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
