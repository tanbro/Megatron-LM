{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron ForumQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD\n",
    "\n",
    "定位到工作目录，根据具体情况决定哦，不一定是下面的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "准备运行这个笔记本的 Jupyter kernel(**如果已经准备就绪，不要重复执行！**)：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 配置一个 Conda 环境作为 Jupyter Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda env update -f environments/environment-ipy.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装完毕后，为该 Notebook 选择这个 Kernel (名为`Megatron_LM-ipy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在Kernel所在 Conda 环境中安装 Apex\n",
    "\n",
    "需要通过 pip 从 github 下载源代码安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -v -r requirements/apex.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 Checkpoints\n",
    "\n",
    "文件比较大，根据实际情况选择下载，**不要重复下载**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "S3_BUCKET = 'huamei'\n",
    "CKPTS_DIR = 'checkpoints/345m-mildil'\n",
    "\n",
    "S3_CKPTS_DIR = 's3://' + os.path.join(S3_BUCKET, CKPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint iter_230000\n",
      "sync: s3://huamei/checkpoints/345m-hmwebmix-bpe-v2/iter_230000 -> checkpoints/345m-hmwebmix-bpe-v2/iter_230000\n",
      "CPU times: user 27.4 ms, sys: 21.1 ms, total: 48.5 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 复制 latest_checkpointed_iteration.txt\n",
    "!aws s3 cp \\\n",
    "    {S3_CKPTS_DIR} \\\n",
    "    {CKPTS_DIR} \\\n",
    "    --recursive \\\n",
    "    --exclude \"*\" \\\n",
    "    --include \"latest_checkpointed_iteration.txt\"\n",
    "\n",
    "# 下载后读取最新的 checkpoint iter 名称\n",
    "iter_step = open(f'{CKPTS_DIR}/latest_checkpointed_iteration.txt').read().strip()\n",
    "ckpt_dir = f'iter_{iter_step}'\n",
    "\n",
    "print(f'checkpoint {ckpt_dir}')\n",
    "\n",
    "s3_ckpt_dir = os.path.join(S3_CKPTS_DIR, ckpt_dir)\n",
    "local_ckpt_dir = os.path.join(CKPTS_DIR, ckpt_dir)\n",
    "\n",
    "print(f'sync: {s3_ckpt_dir} -> {local_ckpt_dir}')\n",
    "    \n",
    "# 同步最新的 Checkpiont\n",
    "!aws s3 sync \\\n",
    "    s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/iter_0230000 \\\n",
    "    ./checkpoints/345m-hmwebmix-bpe-v2/iter_0230000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用哪个/些 GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "from itertools import chain, compress\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from pretrain_gpt2 import get_masks_and_position_ids\n",
    "from predict_gpt2 import initialize_distributed, prepare_tokenizer, set_random_seed, setup_model, get_token_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_HPARAMS_DICT = {\n",
    "    '117m': dict(\n",
    "        num_layers=12,\n",
    "        hidden_size=768,\n",
    "        num_attention_heads=12,\n",
    "        max_position_embeddings=1024,\n",
    "    ),\n",
    "    '345m': dict(\n",
    "        num_layers=24,\n",
    "        hidden_size=1024,\n",
    "        num_attention_heads=16,\n",
    "        max_position_embeddings=1024,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = '117m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=1024,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "#     tokenizer_type='GPT2BPETokenizer_CN',\n",
    "    tokenizer_type='SentencePieceTokenizer',\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    cache_dir=None,\n",
    "    # Training arguments.\n",
    "    load='./checkpoints/xinliqa.117m/',\n",
    "    seed=1234,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    resume_dataloader=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None, \n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "    out_seq_length=1024,\n",
    ")\n",
    "\n",
    "for k, v in MODELS_HPARAMS_DICT[PARAMETERS].items():\n",
    "    setattr(args, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_checkpointed_iteration: 130000\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(args.load, 'latest_checkpointed_iteration.txt')) as fp:\n",
    "    latest_checkpointed_iteration = int(fp.read().strip())\n",
    "print('latest_checkpointed_iteration:', latest_checkpointed_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化函数/全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def initialize():\n",
    "    global model, tokenizer\n",
    "\n",
    "    # Disable CuDNN.\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Pytorch distributed.\n",
    "    initialize_distributed(args)\n",
    "\n",
    "    # Random seeds for reproducability.\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    # get the tokenizer\n",
    "    tokenizer = prepare_tokenizer(args)\n",
    "\n",
    "    # Model, optimizer, and learning rate.\n",
    "    model = setup_model(args)\n",
    "\n",
    "    args.device = torch.cuda.current_device()\n",
    "\n",
    "    # setting default batch size to 1\n",
    "    args.batch_size = 1\n",
    "\n",
    "    assert mpu.get_model_parallel_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主进程初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "prepare tokenizer done\n",
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 0: 110516736\n",
      "global rank 0 is loading checkpoint ./checkpoints/xinliqa.117m/iter_0130000/mp_rank_00/model_optim_rng.pt\n",
      "  successfully loaded ./checkpoints/xinliqa.117m/iter_0130000/mp_rank_00/model_optim_rng.pt\n",
      "CPU times: user 5.22 s, sys: 1.59 s, total: 6.81 s\n",
      "Wall time: 6.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tokens_generative(context_tokens, model, tokenizer):\n",
    "    context_length = len(context_tokens)\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)   \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        yield ids[-1]\n",
    "\n",
    "\n",
    "def infer_text_generative(contex_text, model, tokenizer):\n",
    "    contex_text = contex_text.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(contex_text).tokenization\n",
    "    context_length = len(context_tokens)\n",
    "\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "    \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        s = tokenizer.DecodeIds([ids[-1]])\n",
    "        yield s\n",
    "\n",
    "\n",
    "def question_to_ids(data, end_with_bos=True):\n",
    "    result = []\n",
    "    # 标题, 问题\n",
    "    for k in ('title', 'text'):\n",
    "        s = data[k].strip()\n",
    "        ids = tokenizer.EncodeAsIds(s.strip()).tokenization\n",
    "        result.extend(ids)\n",
    "        result.append(tokenizer.TokenToId('<sep>'))\n",
    "    tags = data.get('tags')\n",
    "    if tags:\n",
    "        tag = random.choice(tags)\n",
    "        ids = tokenizer.EncodeAsIds(tag.strip()).tokenization\n",
    "        result.extend(ids)\n",
    "    result.append(tokenizer.TokenToId('<sep>'))\n",
    "    # 问题/回答 的分隔符，保证连续两个 `<sep>`\n",
    "    result.append(tokenizer.TokenToId('<sep>'))\n",
    "    # 回答 的开头 `<bos>`\n",
    "    if end_with_bos:\n",
    "        result.append(tokenizer.TokenToId('<bos>'))\n",
    "    #\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证是否可运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = [\n",
    "    {\n",
    "        \"title\": \"22岁，做直销\",\n",
    "        \"text\": \"父母都很支持，可是我性格内向，而且不喜欢说话，圈内有很多成功了的大神，我没自信能不能成功，直销靠谱不\",\n",
    "        \"tags\": [\"职业\", \"职业管理\", \"工作压力\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"真理都是相通的?\",\n",
    "        \"text\": \"从洞穴出来看见理性之光的形而上人开始拼凑真理的碎片，到达二点零的人拼凑起来啦一部分碎片形成自己一套看世界的体系。\\n这时候的人都会用自己的一套概念体系去描述他研究得那一部分真理，可是你会发现他们说得都是一回事——真理都是相通的只是表述不同罢啦\\n人性是人文科学的研究对象，物性是自然科学的研究对象，前者合乎情理（符合哲理），后者合乎逻辑（符合形式逻辑那一套），宗教的研究对象是信仰，哲学是哲学家，美学是艺术家。\\n真理的高山一直在那里，我们已经探索啦很多很多，我们学习学科历史是要把山头探索清楚然后摸清楚魔方块的套路，最后构建自己的魔方块\",\n",
    "        \"tags\": [\"科普\", \"书籍\", \"热点话题\", \"心理咨询\"]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1: 难,领导请著我离家等,可能跟其他人讲说话又会不好使,气氛尴尬内向 <eos>\n",
      "Answer 2: 社区都说我是金金立难变。上班暑假过后面试对象,最近,家人对我越来冷淡,踢自行车我也踢了好多要求我去做ay不支持心理学将来,我没有成年学习伙伴,他们就更纯涉之中的,我还大学如何他们都帮助我,后来我都想从一年半前做成为母女,现在他们怕了我朋友跟她妈残害了她有很严重的疾病,我都觉得自己很g很可怕,我很是为别人而活 <eos>\n",
      "Answer 3: ,可我却吃得是挺辛苦的标志真的很不舒服,但就是我越不想在家表面上大看上去很好看,支持我做郭黑车没出息,而我却2号得了,越多鬱,女儿现在才11一再的信任学校,就更不想来了,怎么调解那种他们?我刚开始发现绯闻有人在窥视我隐私特别的活跃的想法!自从女儿玩,看见女孩子染上自拍而且,脸大了,歟26调都丑多了!这是怎么能判断点儿反应?这我一直在儿子轮流,今年若干年裤結囿于即,2015年打算一骄傲点,感受点浓厚和功利,使聪明反思甚至在他们面前下摇。原来我一直所在的院子里,包括医药管也没人能做出来的,但是我妈,那个项目全丢了。我们钱也是全贴补十八万1.2万可以换回,甚至如果换起来方便更多从头再来一次的资金投诉等,我必须一口气把这个还完。其实我也想过向朋友倾诉,可是因为在她们面前谁都没说,等她们哭的那久终于垮了,只有我自己在电话里跟自己说,现在的社会好黑暗,好想改善现状,想挣脱现状,好想赚钱。与丈夫此刻的相处更糟糕。 <eos>\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Answer 1: 像我,让瓣也也被似乎与外师美的面孔哗众取笑,辄主义,不知公主病流不止于夸张,不服输,看法的女孩子和生物之间的竞争我对这方面真不好吧, <eos>\n",
      "Answer 2: 人心都是个神经元,不过宣传酷伦后来我所在,暗恋过度解读,因为中国式思想和思想们不断步死亡,我被感染但是我就和中国系统学号说如果人中医学不好也提高了。这是我想的?怎样自我探究?有时候那个人说的我很新奇想的一些人,其实有些不完全,但是这是他非的心理学依据???我怕他们注视不是医学专业。。。他们老让我自言自语啦,我该怎么办?求解释 <eos>\n",
      "Answer 3: 人心目,我的样样蛇女判断力极好,否则不敢得罪人...夜深人,和机器人曾遭遇过強的磨合,学生都很多..而且亲口说活在世,但身边的人便一直隐忍着...这十年,幻听而八面阅读量表,在青少年遭受来自各方面的处罚被迫害妄想症,大多数只在在自己的房间曾神奇的是,他还有意识的这个梦是什么意思? <eos>\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.recompute=True\n",
    "# args.top_p=0.0\n",
    "# args.top_k=0\n",
    "# args.temperature=1.0\n",
    "\n",
    "n_gen = 3\n",
    "max_try = 3\n",
    "\n",
    "for question in question_list:\n",
    "#     print(question['title'])\n",
    "#     print(question['text'])\n",
    "#     input_ids = question_to_ids(question)\n",
    "#     print(tokenizer.DecodeIds(input_ids))\n",
    "#     print()\n",
    "\n",
    "    for i in range(n_gen):\n",
    "#         args.temperature=random.gauss(0.95, 0.05)\n",
    "        print(f'Answer {i+1}: ', end='')\n",
    "        s = ''\n",
    "        for _ in range(max_try):\n",
    "            if s:\n",
    "                break\n",
    "            input_ids = question_to_ids(question)\n",
    "            for j, id_ in enumerate(infer_tokens_generative(input_ids, model, tokenizer)):\n",
    "                if j == 0:\n",
    "#                     if id_ != tokenizer.TokenToId('<bos>'):\n",
    "#                         break\n",
    "                    if id_ == tokenizer.TokenToId('<eos>'):\n",
    "                        break\n",
    "                s = tokenizer.DecodeIds([id_])\n",
    "                print(s, end='')\n",
    "        print()\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 test 语料，从中随机打断，并预测下文，比较原文与预测结果！\n",
    "\n",
    "随机选 N 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_file=./data/xinliqa/117m.predicted-130000-1000_1024_5.random.shuffle.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac8489d5c544811bed7c8e33c96ebe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 数据总数: 4,000\n",
      "Test 采样数: 1,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551758e30e014974989a6560b7e07458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='sample', max=1000, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d19a2fb53674de09615c549da736b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='infer', max=1000, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "n_infer = 5\n",
    "b_random = True\n",
    "b_shuffle = True\n",
    "\n",
    "input_file = './data/xinliqa/test.json'\n",
    "output_file = f'./data/xinliqa/{PARAMETERS}.predict-{latest_checkpointed_iteration}-{n_samples}_{args.seq_length}_{n_infer}'\n",
    "if b_random:\n",
    "    output_file += '.random'\n",
    "if b_shuffle:\n",
    "    output_file += '.shuffle'\n",
    "output_file += '.json'\n",
    "\n",
    "print(f'output_file={output_file}')\n",
    "\n",
    "total = sum(1 for _ in tqdm(open(input_file)))\n",
    "print(f'Test 数据总数: {total:,d}')\n",
    "\n",
    "if n_samples > 0:\n",
    "    assert total >= n_samples\n",
    "else:\n",
    "    n_samples = total\n",
    "\n",
    "print(f'Test 采样数: {n_samples:,d}')\n",
    "\n",
    "mask = np.zeros(total, dtype=int)\n",
    "mask[:n_samples] = 1\n",
    "if b_random:\n",
    "    np.random.shuffle(mask)\n",
    "\n",
    "samples = []\n",
    "with open(input_file) as fp:\n",
    "    reader = compress(fp, mask)\n",
    "    for line in tqdm(reader, 'sample', total=n_samples):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "        samples.append(data)\n",
    "\n",
    "if b_shuffle:\n",
    "    random.shuffle(samples)\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    for data in tqdm(samples, 'infer'):\n",
    "        inferred = data['inferred'] = []\n",
    "        for i in range(n_infer):\n",
    "            input_ids = question_to_ids(data)\n",
    "            output_ids = list(infer_tokens_generative(input_ids, model, tokenizer))\n",
    "            inferred.append(tokenizer.DecodeIds(output_ids))\n",
    "        print(json.dumps(data, ensure_ascii=False), file=fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
