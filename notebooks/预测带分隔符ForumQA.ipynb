{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron ForumQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD\n",
    "\n",
    "定位到工作目录，根据具体情况决定哦，不一定是下面的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "准备运行这个笔记本的 Jupyter kernel(**如果已经准备就绪，不要重复执行！**)：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 配置一个 Conda 环境作为 Jupyter Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda env update -f environments/environment-ipy.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装完毕后，为该 Notebook 选择这个 Kernel (名为`Megatron_LM-ipy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在Kernel所在 Conda 环境中安装 Apex\n",
    "\n",
    "需要通过 pip 从 github 下载源代码安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -v -r requirements/apex.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 Checkpoints\n",
    "\n",
    "文件比较大，根据实际情况选择下载，**不要重复下载**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "S3_BUCKET = 'huamei'\n",
    "CKPTS_DIR = 'checkpoints/345m-mildil'\n",
    "\n",
    "S3_CKPTS_DIR = 's3://' + os.path.join(S3_BUCKET, CKPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint iter_230000\n",
      "sync: s3://huamei/checkpoints/345m-hmwebmix-bpe-v2/iter_230000 -> checkpoints/345m-hmwebmix-bpe-v2/iter_230000\n",
      "CPU times: user 27.4 ms, sys: 21.1 ms, total: 48.5 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 复制 latest_checkpointed_iteration.txt\n",
    "!aws s3 cp \\\n",
    "    {S3_CKPTS_DIR} \\\n",
    "    {CKPTS_DIR} \\\n",
    "    --recursive \\\n",
    "    --exclude \"*\" \\\n",
    "    --include \"latest_checkpointed_iteration.txt\"\n",
    "\n",
    "# 下载后读取最新的 checkpoint iter 名称\n",
    "iter_step = open(f'{CKPTS_DIR}/latest_checkpointed_iteration.txt').read().strip()\n",
    "ckpt_dir = f'iter_{iter_step}'\n",
    "\n",
    "print(f'checkpoint {ckpt_dir}')\n",
    "\n",
    "s3_ckpt_dir = os.path.join(S3_CKPTS_DIR, ckpt_dir)\n",
    "local_ckpt_dir = os.path.join(CKPTS_DIR, ckpt_dir)\n",
    "\n",
    "print(f'sync: {s3_ckpt_dir} -> {local_ckpt_dir}')\n",
    "    \n",
    "# 同步最新的 Checkpiont\n",
    "!aws s3 sync \\\n",
    "    s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/iter_0230000 \\\n",
    "    ./checkpoints/345m-hmwebmix-bpe-v2/iter_0230000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用哪个/些 GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "from itertools import chain, compress\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from pretrain_gpt2 import get_masks_and_position_ids\n",
    "from predict_gpt2 import initialize_distributed, prepare_tokenizer, set_random_seed, setup_model, get_token_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_HPARAMS_DICT = {\n",
    "    '117m': dict(\n",
    "        num_layers=12,\n",
    "        hidden_size=768,\n",
    "        num_attention_heads=12,\n",
    "        max_position_embeddings=1024,\n",
    "    ),\n",
    "    '345m': dict(\n",
    "        num_layers=24,\n",
    "        hidden_size=1024,\n",
    "        num_attention_heads=16,\n",
    "        max_position_embeddings=1024,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = '117m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=1024,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "#     tokenizer_type='GPT2BPETokenizer_CN',\n",
    "    tokenizer_type='SentencePieceTokenizer',\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    cache_dir=None,\n",
    "    # Training arguments.\n",
    "    load='./checkpoints/xinliqa.117m/',\n",
    "    seed=1234,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    resume_dataloader=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None, \n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "    out_seq_length=1024,\n",
    ")\n",
    "\n",
    "for k, v in MODELS_HPARAMS_DICT[PARAMETERS].items():\n",
    "    setattr(args, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_checkpointed_iteration: 180000\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(args.load, 'latest_checkpointed_iteration.txt')) as fp:\n",
    "    latest_checkpointed_iteration = int(fp.read().strip())\n",
    "print('latest_checkpointed_iteration:', latest_checkpointed_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化函数/全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def initialize():\n",
    "    global model, tokenizer\n",
    "\n",
    "    # Disable CuDNN.\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Pytorch distributed.\n",
    "    initialize_distributed(args)\n",
    "\n",
    "    # Random seeds for reproducability.\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    # get the tokenizer\n",
    "    tokenizer = prepare_tokenizer(args)\n",
    "\n",
    "    # Model, optimizer, and learning rate.\n",
    "    model = setup_model(args)\n",
    "\n",
    "    args.device = torch.cuda.current_device()\n",
    "\n",
    "    # setting default batch size to 1\n",
    "    args.batch_size = 1\n",
    "\n",
    "    assert mpu.get_model_parallel_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主进程初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "prepare tokenizer done\n",
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 0: 110516736\n",
      "global rank 0 is loading checkpoint ./checkpoints/xinliqa.117m/iter_0180000/mp_rank_00/model_optim_rng.pt\n",
      "  successfully loaded ./checkpoints/xinliqa.117m/iter_0180000/mp_rank_00/model_optim_rng.pt\n",
      "CPU times: user 5.19 s, sys: 1.62 s, total: 6.81 s\n",
      "Wall time: 6.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tokens_generative(context_tokens, model, tokenizer):\n",
    "    context_length = len(context_tokens)\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)   \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        yield ids[-1]\n",
    "\n",
    "\n",
    "def infer_text_generative(contex_text, model, tokenizer):\n",
    "    contex_text = contex_text.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(contex_text).tokenization\n",
    "    context_length = len(context_tokens)\n",
    "\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "    \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        s = tokenizer.DecodeIds([ids[-1]])\n",
    "        yield s\n",
    "\n",
    "\n",
    "def question_to_ids(data, end_with_bos=True):\n",
    "    result = []\n",
    "    # 标题, 问题\n",
    "    for k in ('title', 'text'):\n",
    "        s = data[k].strip()\n",
    "        ids = tokenizer.EncodeAsIds(s.strip()).tokenization\n",
    "        result.extend(ids)\n",
    "        result.append(tokenizer.TokenToId('<sep>'))\n",
    "    tags = data.get('tags')\n",
    "    if tags:\n",
    "        tag = random.choice(tags)\n",
    "        ids = tokenizer.EncodeAsIds(tag.strip()).tokenization\n",
    "        result.extend(ids)\n",
    "    result.append(tokenizer.TokenToId('<sep>'))\n",
    "    # 问题/回答 的分隔符，保证连续两个 `<sep>`\n",
    "    result.append(tokenizer.TokenToId('<sep>'))\n",
    "    # 回答 的开头 `<bos>`\n",
    "    if end_with_bos:\n",
    "        result.append(tokenizer.TokenToId('<bos>'))\n",
    "    #\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证是否可运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = [\n",
    "    {\n",
    "        \"title\": \"22岁，做直销\",\n",
    "        \"text\": \"父母都很支持，可是我性格内向，而且不喜欢说话，圈内有很多成功了的大神，我没自信能不能成功，直销靠谱不\",\n",
    "        \"tags\": [\"职业\", \"职业管理\", \"工作压力\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"真理都是相通的?\",\n",
    "        \"text\": \"从洞穴出来看见理性之光的形而上人开始拼凑真理的碎片，到达二点零的人拼凑起来啦一部分碎片形成自己一套看世界的体系。\\n这时候的人都会用自己的一套概念体系去描述他研究得那一部分真理，可是你会发现他们说得都是一回事——真理都是相通的只是表述不同罢啦\\n人性是人文科学的研究对象，物性是自然科学的研究对象，前者合乎情理（符合哲理），后者合乎逻辑（符合形式逻辑那一套），宗教的研究对象是信仰，哲学是哲学家，美学是艺术家。\\n真理的高山一直在那里，我们已经探索啦很多很多，我们学习学科历史是要把山头探索清楚然后摸清楚魔方块的套路，最后构建自己的魔方块\",\n",
    "        \"tags\": [\"科普\", \"书籍\", \"热点话题\", \"心理咨询\"]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1: ,我都会嘲笑我想,见识的很二三任咨询师如何都只会做着变成说过话,真到明了,但我正用穿着女性脱精液来毁容取女,我感觉很反感。总是一个家店女同事衣着穿,三观逐步起来,我发现,脸色差不多,随着年龄的增长,脾气比较萌,也越来越像她吹毛求疵,我也不是邪,最重要的是小孩做不到,我知道这样让自己疑神病一样的,我家人说是妹妹让我不要把孩子气剪的,于是我拼命的抓,不仅不听话还天天说我在那里等着,还动不动说别的小孩,我看着不舒服,我都快气疯了,我要打我妈了。对了,说我是青春期的女生,那我再怎么长身体呢,应该和她一样傻傻的接受治疗吗?作为子女,我该怎么做? <eos>\n",
      "Answer 2: 事社团,我却收益,我不知道初中的人资视乐都说不出会就长大些太少,比较像小的学生、大方选采购。从开学到现在不了解,不知道为什么会不知道为什么,每次你我来提问到现在,我常常问兵老神从其正确答,他说结婚哟我以为是,蜜蜂蜜,做普工的像,给他女儿画笔快乐。可是,后妈姐姐弟弟态度很亢奋,严重影响了学习考大学等,出来工作几年,也为了收益累积,拒绝回家。我很少在公开场合发言,大学期间偶尔亲亲昵撒娇,也被拉黑,以致那段时间被当众演讲的面试管说我是傻13名,从此我就有了对自己不满意的疏远情绪,为了避免来往还当面冷场。后来出院后,我主动热情的接待了她,说我是脑子有问题,然后她问了一句话让我闭嘴。我其实活得很累,有一点胆小,害羞,缺少自信心,我很怕说出错让她没面子,所以很怕打击我,在最喜欢的人面前不敢直视对方(因为对方说话的语句时常很急,扭头缩脚的,因为害怕拒绝而装不下去说错话让对方不乐意),所以我很纠结,我也因为心急,说双方父母这边的帮朋友同事不是自身,而是多余的人互相交流,互相了解答题,做事情很多不像关心。我很迷茫,感觉自己越来越没有主见了,就陷入了死循环。 <eos>\n",
      "Answer 3: 的爸爸,不到就我们控诉说,所以,可是没人这样的人能年长女孩子高,我们在一起了快三年,她在我知道了,但是我想学识到储蓄咨询师,我和舅舅家距离最远,有点去过不了,要不然他不懂钱财在棵树上诸位。认识的人都长的蛮好的,接受的比自己有钱人多的多,都在省钱,我知道,但是她人言可畏,刚刚还做了一个很大的那个阿姨。我虽然很喜欢和她们住在一起,但由于我也知道母亲并不爱她,她将心里的感觉完全和她们疏远,过了几天,我们就因为这件事情讨论得很剧烈,想提建议,她说觉得很累,不想多想,可高兴不起来,现在我们心里都很难受,想请教一下怎样对待和朋友的建议,怎样不那么敏感,怎样能够保持好心情? <eos>\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Answer 1: (全球师大学能够用来做比赛来参加的知识空间什么的),可以做现实中对应的人爱!我感觉现在好好的,以后不许看别人,但是这种想法言论是最恐怖的 <eos>\n",
      "Answer 2: (1%都是3年华凭什么),我是女生因为紧张、许多人她在热火朝我转,m。的时候只能一下子找出平时的自己,是一个最柔最长的一道基础之一,自杀失败以后失败从此以后更抑郁,不相信心理医生,有,,请问有人能够解这样的人吗? <eos>\n",
      "Answer 3: ,日本参加咨询片,白天的班各种概念学心理学就是载体的变化。5年来曾催眠史,却是在网络中喜欢着一个人学习,现实中总是幻想很多美好的东西,好像没有能力而导致两个人的精神空虚)的负面情绪开始失去自我???心态不是很好吧...... <eos>\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.recompute=True\n",
    "# args.top_p=0.0\n",
    "# args.top_k=0\n",
    "# args.temperature=1.0\n",
    "\n",
    "n_gen = 3\n",
    "max_try = 3\n",
    "\n",
    "for question in question_list:\n",
    "#     print(question['title'])\n",
    "#     print(question['text'])\n",
    "#     input_ids = question_to_ids(question)\n",
    "#     print(tokenizer.DecodeIds(input_ids))\n",
    "#     print()\n",
    "\n",
    "    for i in range(n_gen):\n",
    "#         args.temperature=random.gauss(0.95, 0.05)\n",
    "        print(f'Answer {i+1}: ', end='')\n",
    "        s = ''\n",
    "        for _ in range(max_try):\n",
    "            if s:\n",
    "                break\n",
    "            input_ids = question_to_ids(question)\n",
    "            for j, id_ in enumerate(infer_tokens_generative(input_ids, model, tokenizer)):\n",
    "                if j == 0:\n",
    "#                     if id_ != tokenizer.TokenToId('<bos>'):\n",
    "#                         break\n",
    "                    if id_ == tokenizer.TokenToId('<eos>'):\n",
    "                        break\n",
    "                s = tokenizer.DecodeIds([id_])\n",
    "                print(s, end='')\n",
    "        print()\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 test 语料，从中随机打断，并预测下文，比较原文与预测结果！\n",
    "\n",
    "随机选 N 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_file=./data/xinliqa/117m.predicted-130000-1000_1024_5.random.shuffle.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac8489d5c544811bed7c8e33c96ebe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 数据总数: 4,000\n",
      "Test 采样数: 1,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551758e30e014974989a6560b7e07458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='sample', max=1000, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d19a2fb53674de09615c549da736b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='infer', max=1000, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "n_infer = 5\n",
    "b_random = True\n",
    "b_shuffle = True\n",
    "\n",
    "input_file = './data/xinliqa/test.json'\n",
    "output_file = f'./data/xinliqa/{PARAMETERS}.predict-{latest_checkpointed_iteration}-{n_samples}_{args.seq_length}_{n_infer}'\n",
    "if b_random:\n",
    "    output_file += '.random'\n",
    "if b_shuffle:\n",
    "    output_file += '.shuffle'\n",
    "output_file += '.json'\n",
    "\n",
    "print(f'output_file={output_file}')\n",
    "\n",
    "total = sum(1 for _ in tqdm(open(input_file)))\n",
    "print(f'Test 数据总数: {total:,d}')\n",
    "\n",
    "if n_samples > 0:\n",
    "    assert total >= n_samples\n",
    "else:\n",
    "    n_samples = total\n",
    "\n",
    "print(f'Test 采样数: {n_samples:,d}')\n",
    "\n",
    "mask = np.zeros(total, dtype=int)\n",
    "mask[:n_samples] = 1\n",
    "if b_random:\n",
    "    np.random.shuffle(mask)\n",
    "\n",
    "samples = []\n",
    "with open(input_file) as fp:\n",
    "    reader = compress(fp, mask)\n",
    "    for line in tqdm(reader, 'sample', total=n_samples):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "        samples.append(data)\n",
    "\n",
    "if b_shuffle:\n",
    "    random.shuffle(samples)\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    for data in tqdm(samples, 'infer'):\n",
    "        inferred = data['inferred'] = []\n",
    "        for i in range(n_infer):\n",
    "            input_ids = question_to_ids(data)\n",
    "            output_ids = list(infer_tokens_generative(input_ids, model, tokenizer))\n",
    "            inferred.append(tokenizer.DecodeIds(output_ids))\n",
    "        print(json.dumps(data, ensure_ascii=False), file=fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
