{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron 直接使用预训练模型进行预测 (BPE v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "准备运行这个笔记本的 Jupyter kernel(**如果已经准备就绪，不要重复执行！**)：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 配置一个 Conda 环境作为 Jupyter Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda env update -f environments/environment-ipy.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装完毕后，为该 Notebook 选择这个 Kernel (名为`Megatron_LM-ipy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在Kernel所在 Conda 环境中安装 Apex\n",
    "\n",
    "需要通过 pip 从 github 下载源代码安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -v -r requirements/apex.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD\n",
    "\n",
    "定位到工作目录，根据具体情况决定哦，不一定是下面的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 Checkpoints\n",
    "\n",
    "文件比较大，根据实际情况选择下载，**不要重复下载**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "S3_BUCKET = 'huamei'\n",
    "CKPTS_DIR = 'checkpoints/345m-hmwebmix-bpe-v2'\n",
    "\n",
    "S3_CKPTS_DIR = 's3://' + os.path.join(S3_BUCKET, CKPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint iter_230000\n",
      "sync: s3://huamei/checkpoints/345m-hmwebmix-bpe-v2/iter_230000 -> checkpoints/345m-hmwebmix-bpe-v2/iter_230000\n",
      "CPU times: user 27.4 ms, sys: 21.1 ms, total: 48.5 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 复制 latest_checkpointed_iteration.txt\n",
    "!aws s3 cp \\\n",
    "    {S3_CKPTS_DIR} \\\n",
    "    {CKPTS_DIR} \\\n",
    "    --recursive \\\n",
    "    --exclude \"*\" \\\n",
    "    --include \"latest_checkpointed_iteration.txt\"\n",
    "\n",
    "# 下载后读取最新的 checkpoint iter 名称\n",
    "iter_step = open(f'{CKPTS_DIR}/latest_checkpointed_iteration.txt').read().strip()\n",
    "ckpt_dir = f'iter_{iter_step}'\n",
    "\n",
    "print(f'checkpoint {ckpt_dir}')\n",
    "\n",
    "s3_ckpt_dir = os.path.join(S3_CKPTS_DIR, ckpt_dir)\n",
    "local_ckpt_dir = os.path.join(CKPTS_DIR, ckpt_dir)\n",
    "\n",
    "print(f'sync: {s3_ckpt_dir} -> {local_ckpt_dir}')\n",
    "    \n",
    "# 同步最新的 Checkpiont\n",
    "!aws s3 sync \\\n",
    "    s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/iter_0230000 \\\n",
    "    ./checkpoints/345m-hmwebmix-bpe-v2/iter_0230000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用哪个/些 GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "from itertools import chain, compress\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from pretrain_gpt2 import get_masks_and_position_ids\n",
    "from predict_gpt2 import initialize_distributed, prepare_tokenizer, set_random_seed, setup_model, get_token_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    num_layers=24,\n",
    "    hidden_size=1024,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=1024,\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=1024,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "    tokenizer_type='GPT2BPETokenizer_CN',\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    cache_dir=None,\n",
    "    # Training arguments.\n",
    "    load='./checkpoints/345m-hmwebmix-bpe-v2/',\n",
    "    seed=1234,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    resume_dataloader=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None, \n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "    out_seq_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化函数/全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def initialize():\n",
    "    global model, tokenizer\n",
    "\n",
    "    # Disable CuDNN.\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Pytorch distributed.\n",
    "    initialize_distributed(args)\n",
    "\n",
    "    # Random seeds for reproducability.\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    # get the tokenizer\n",
    "    tokenizer = prepare_tokenizer(args)\n",
    "\n",
    "    # Model, optimizer, and learning rate.\n",
    "    model = setup_model(args)\n",
    "\n",
    "    args.device = torch.cuda.current_device()\n",
    "\n",
    "    # setting default batch size to 1\n",
    "    args.batch_size = 1\n",
    "\n",
    "    assert mpu.get_model_parallel_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主进程初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "prepare tokenizer done\n",
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 0: 336128000\n",
      "global rank 0 is loading checkpoint ./checkpoints/345m-hmwebmix-bpe-v2/iter_0230000/mp_rank_00/model_optim_rng.pt\n",
      "  successfully loaded ./checkpoints/345m-hmwebmix-bpe-v2/iter_0230000/mp_rank_00/model_optim_rng.pt\n",
      "CPU times: user 11.8 s, sys: 3.58 s, total: 15.4 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tokens_generative(context_tokens, model, tokenizer):\n",
    "    context_length = len(context_tokens)\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)   \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        yield ids[-1]\n",
    "\n",
    "\n",
    "def infer_text_generative(contex_text, model, tokenizer):\n",
    "    contex_text = contex_text.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(contex_text).tokenization\n",
    "    context_length = len(context_tokens)\n",
    "\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "    \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        s = tokenizer.DecodeIds([ids[-1]])\n",
    "        yield s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测试试看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我喜欢\n",
      "==========\n",
      "\n",
      "(1)/3\t刘涛,不喜欢她和高圆圆。为什么我喜欢高圆圆离开后,那个高圆圆会这么空洞。其实不是某个神奇而迷人的女性,因为每个人心中都有一个高圆圆,如果愿意,是自己对她的刻意追求以及自卑。高圆圆夺得金鹰女神之后,真实的她是多么好奇怪。先来做些科普吧。高圆圆最出名的笑容是看着别人的眼睛露出温柔,似乎这个世上还没有一个人比她更温柔了,仿佛的眼睛里只剩下柔情蜜意。这是美人如瓶,骨如锉的特点。重点来了,你以为的\n",
      "\n",
      "(2)/3\t这台曾经大闹画拍卖的相机,首先是因为它保养我父亲13年的时间。用3年的时间拿到一块当时全国产相机里最高的水准:3500像素F1.7广角、3500像素长焦以及100万像素4K清胶片。与其他入门级相机相比,F1.7的画质新奇,而且更加配备许多功能强大的人像模式,OCAP机型的STMMN动画可以连续撕下电池底座,让画面更加美轮美奂。首先,单击相机菜单的Joy-Conn\n",
      "\n",
      "(3)/3\t这样的漫漫长夜,但我还是忍不住第二场剧目的巡演邀请他们,让我走出深渊,去彼此看一个“有故事”(对于女主来说)的夜色,所以我的生意就越来越多。后台一直在说女孩子长大不是很漂亮,但常常在粉丝面前得到一句抱歉,如果是一个男孩子,他会说这句话,让我此刻内心平静。跑圈,上验光,做手术,甚至拍走路的戏要求也变多了,对剧知道的越来越多,对角色也越来越熟悉这个时候的《太子妃升职记》找到我,找到我想做演员,也很满意。但是之前\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contex_text = '我喜欢'\n",
    "\n",
    "print(contex_text)\n",
    "print('==========')\n",
    "print()\n",
    "\n",
    "N = 3\n",
    "for i in range(N):\n",
    "    print(f'({1+i})/{N}\\t', end='')\n",
    "    for s in infer_text_generative(contex_text.strip(), model, tokenizer):\n",
    "        print(s, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对一组心理/情绪相关文字进行续写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    '宝贝“啃指甲”是为什么呢？宝贝是缺铁、缺锌了吗？宝贝是肚子里有虫吗？这指甲里好多细菌宝宝都吃下去怎么是好？怎么能让宝宝不“啃指甲”呢？家长应该怎么应对呢？',\n",
    "    '几年前，英国就有专家向国会递交报告，建议政府制定法律禁止家长让年龄低于3岁的儿童看电视。奇幻小说《查理和巧克力工厂》有句台词：“千万、千万、千万别让孩子，靠近你的电视，最好是别购买、安装，这最最愚蠢的东西”。',\n",
    "    '婚姻是一座炼狱，因为在这场婚姻里夹杂着各种人际关系，经过这些人际关系，人才能慢慢心智成熟起来。而能够把这些人际关系处理好的，往往都是生活中的高手。那么夫妻双方该如何经营婚姻呢？这里就提供一些方法供参考。',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宝贝“啃指甲”是为什么呢？宝贝是缺铁、缺锌了吗？宝贝是肚子里有虫吗？这指甲里好多细菌宝宝都吃下去怎么是好？怎么能让宝宝不“啃指甲”呢？家长应该怎么应对呢？\n",
      "\n",
      "0: 是教孩子养成好习惯还是强迫孩子多吃点零食?通常,初二生长的宝宝,特爱心高胆大性情急躁,因此,家长最好就不要强迫孩子啃指甲。可物理干燥除甲坏习惯也或可引起阴痒影响食欲,增加肠胃负担,应该在散完水分后即用富含碘的食物涂擦,有些孩子用手抚摸指甲,往往会抓破手指皮肤,而产生细菌感染。家长还应教给孩子注意卫生,如保持皮肤干燥。如果孩子的指甲很扁平,建议到医院检查一下\n",
      "\n",
      "1: 下面,咱们就一起来看一看吧,有关于宝宝胃肠道人体内是什么细菌的。一、维生素类2岁之前婴幼儿消化道的标本物质构成与成人相同,基本不全面,而且还容易摄入脂肪。同时,大多数微生物都渗透到五官之中,极容易与粘膜结合侵入胃肠道内,引起肠刺激反应缺铁,影响脂肪代谢的主因,大多由缺铁而引起。辅食中的铁元素缺乏,影响脑脊液的合成,容易引起中耳炎等传染病。包囊内有凝集素,\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "几年前，英国就有专家向国会递交报告，建议政府制定法律禁止家长让年龄低于3岁的儿童看电视。奇幻小说《查理和巧克力工厂》有句台词：“千万、千万、千万别让孩子，靠近你的电视，最好是别购买、安装，这最最愚蠢的东西”。\n",
      "\n",
      "0: 有点教育专家认为婴幼儿对应的是摇篮一样,可以说是一个“摇篮大师”,因为婴儿时期的教育是优于成人的。因为婴儿时期的幼儿可以说都是模仿成人的,当儿童逐渐长大,或者婴儿从两个方面,尤其是从身体状况、上幼儿园这些方面可能远不如成人此前,「失去对世界的认识、由此而产生的对外界新的经验以及和成人的交往和交流的能力」,而只有通过父母对婴儿的早期教育,并走向社会而积累下一定的社会资源时,才能有效的培养儿童的这一特点。这个国家人口基数并不大,孩子逗小孩子玩的时间应该\n",
      "\n",
      "1: 那能不能让孩子远离低龄电视?小编死磕专家,凭精神数了数,孩子出自哪里就自行走脑。首先跟大家聊聊在成人眼中的电视这份礼仪。英国的理由比较灵活,有些是出于礼貌仪态,比如电视机的功率要低于路人的照明;有些是别绊着“孩子的眼”,比如上面的问题,如果孩子正好在看电视,请他到旁边站一会儿,一个多小时后再拿回来,流氓关系一下子淡了。看了英国人写进法律的拔高法律要求且符合道德规范的六类规定后,你又多了\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "婚姻是一座炼狱，因为在这场婚姻里夹杂着各种人际关系，经过这些人际关系，人才能慢慢心智成熟起来。而能够把这些人际关系处理好的，往往都是生活中的高手。那么夫妻双方该如何经营婚姻呢？这里就提供一些方法供参考。\n",
      "\n",
      "0: 婚姻是人一生中可能经历的最大痛,是最容易出问题的点。但如果夫妻双方能够正确的经营婚姻,一个问题就可以迎刃而解。来看看很多婚姻中的问题,都是从这里一步步累积到最后爆发的。婚姻不可避免会造成以下几个严重问题:挣得少,逐渐进入恶性循环。大多数中国人似乎认为“挣得少”会给家庭带来风险作用,特别是那种女性体力不如男的家庭,固然婚后会越来越惨。但这完全就是合理的判断吗?近年来,越来越多的研究显示,女子中男性的出问题往往比男性多。2004\n",
      "\n",
      "1: 提起婚姻就想到柴米油盐酱醋茶,抗挤牙膏也众所周知。想想,为什么有的人能修得一份亲密无间,能够把这个家庭经营出的一片和谐与美景。有的人却因为爱的太过于坦诚让整个家庭鸡飞狗跳,或者是鸡犬不宁。同样的道理,想要婚姻中处于一个稳定状态,那么首先就要做好的地方。上帝给予我们人类的第一个能力,就是学习。即使你在当家庭主妇当得并不开心,当自己对这段生活束手无策的时候,这是唯一能从生活中找到可以下一块根据地\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.recompute=True\n",
    "# args.top_p=0.0\n",
    "# args.top_k=0\n",
    "# args.temperature=1.0\n",
    "\n",
    "n_gen = 2\n",
    "\n",
    "for txt in input_texts:\n",
    "    context_tokens = tokenizer.EncodeAsIds(txt).tokenization\n",
    "    print(txt)\n",
    "    print()\n",
    "    for i in range(n_gen):\n",
    "#         args.temperature=random.gauss(0.95, 0.05)\n",
    "        print(f'{i}: ', end='')\n",
    "        s_pred = ''\n",
    "        for s in infer_text_generative(txt, model, tokenizer):\n",
    "            print(s, end='')\n",
    "        print(os.linesep)\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 test 语料，从中随机打断，并预测下文，比较原文与预测结果！\n",
    "\n",
    "随机选 N 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 108124.58it/s]\n",
      "sample: 100%|██████████| 100/100 [00:00<00:00, 1115.87it/s]\n",
      "infer:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_file=./data/hmwebmix/hmwebmix.test.230000.128.100.tsv\n",
      "Test 数据总数: 10,000\n",
      "Test 采样数: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer: 100%|██████████| 100/100 [06:20<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "\n",
    "input_file = './data/hmwebmix/hmwebmix.test.json'\n",
    "output_file = f'./data/hmwebmix/hmwebmix.test.{iter_step}.{args.out_seq_length}.{N}.tsv'\n",
    "\n",
    "print(f'output_file={output_file}')\n",
    "\n",
    "total = sum(1 for _ in tqdm(open(input_file)))\n",
    "print(f'Test 数据总数: {total:,d}')\n",
    "\n",
    "assert total >= N\n",
    "\n",
    "print(f'Test 采样数: {N:,d}')\n",
    "\n",
    "mask = np.zeros(total, dtype=int)\n",
    "mask[:N] = 1\n",
    "np.random.shuffle(mask)\n",
    "\n",
    "samples = []\n",
    "with open(input_file) as fp:\n",
    "    reader = compress(fp, mask)\n",
    "    for line in tqdm(reader, 'sample', total=N):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        text = json.loads(line)['text']\n",
    "        samples.append(text)\n",
    "random.shuffle(samples)\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    writer = csv.writer(fp, delimiter='\\t')\n",
    "    for context_txt in tqdm(samples, 'infer'):\n",
    "        context_ids = tokenizer.EncodeAsIds(context_txt).tokenization\n",
    "        context_len = len(context_ids)\n",
    "        idx = round(random.gauss(context_len*0.5, context_len*0.1))\n",
    "        input_ids = context_ids[:idx]\n",
    "        label_ids = context_ids[idx:]\n",
    "        infer_ids = [id_ for id_ in infer_tokens_generative(input_ids, model, tokenizer)]\n",
    "        row = [\n",
    "            tokenizer.DecodeIds(ids)\n",
    "            for ids in (input_ids, label_ids, infer_ids)\n",
    "        ]\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测一组 QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    (\n",
    "        '最近晚上睡不着觉，但很困，总觉得不会睡觉？',\n",
    "        '最近一段时间，总是晚上睡不着觉，但是却很困，没次睡觉心理都在不自觉的想着该如何去睡觉，每次想着想着就完全清醒了，失眠特别严重，感觉身体快要透支了，很无助，还连累家人一起跟着担心'\n",
    "    ),\n",
    "    (\n",
    "        '大一，谈了将近一年的女友突然提出分手我该怎么办？',\n",
    "        ' 某一天晚上她突然给我打电话说我不是她喜欢的那种类型，在电话中她多次提到自己的性格不好，说“我所看到的都是加了“滤镜”的效果，其实她并没有我想得那么好，今后不准备结婚”。（她以前谈过一次恋爱，对于那场恋爱她一直都铭记在心）接完电话我很迷茫和困惑，因为之前她愿意跟我分享她的生活中的不如意和困难，突然的转变让我措手不及。同时我也很自责，没有准确的表达出自己的意思和情感，在与她的对话中我常常感到无力和无助。我到底该怎么办？我真的很爱她，想和她度过一生。'\n",
    "    ),\n",
    "    (\n",
    "        '男人出轨',\n",
    "        '我的意思是，出轨者会问自己，我为什么要这么做，被背叛的伴侣会问，你为什么要这样对我?'\n",
    "    ),\n",
    "    (\n",
    "        '我的奇葩婚姻，老公说为了孩子好就是不离婚？',\n",
    "        '我的婚姻最近好像一塌糊涂，可是老公不离婚，我甚至有想过死？我是不是得了抑郁症？和老公从2017年开始就吵吵闹闹，一吵架就分居，分居一两个月老公可以对我和孩子不问不闻，到时间了他就自己又搬回来，但是回来也不和我沟通，就算是上次是他动手，他也不道歉，他特别喜欢冷战。现在和老公又发展到了无性，而且双方家庭也合不来，我和他家人不敢来往，他家人对我要求高，希望天天在家做饭带娃伺候老公，他现在和我父母也不来往，从来不回家吃饭，要等我父母走了他才回来，每天很晚回家，去哪从来不告诉我，我知道他有婚外性，但他從來不承認，还说我是神经病，我還被他傳播上了高危型的HPV，以后是死是活都不确定，我特别想要离婚，我感觉和他生活快要窒息了，他整天拉着个脸，要么对我不闻不问，要么一开口说话就是责怪的语气，但他就是不离婚，说是为了孩子好，还让我不要发神经，我真的不知道该怎么和他相处了。他性格很古怪'\n",
    "    ),\n",
    "    (\n",
    "        '妻子在钱的问题上不坦诚，如何处理好家庭经济问题？',\n",
    "        '和老婆因为意外怀孕结婚，两家谈婚论嫁时闹得不愉快，我家出了一套大户型的房子，一台豪华车（当然这些都是婚前财产），然后给了40万装修婚房，同时给女方10万彩礼。而女方一开始的态度是一分钱没有，后来迫于压力给了50万现金，我老婆名下既没有房也没有车。婚后的钱一直都是给我老婆管理，包括我的工资奖金还有结婚的份子钱等等。我老婆并没有把这些钱看做是家庭财产，就算其中已经掺入了很多我的钱，还是认为全部都是她自己的钱，我只是知道她的卡密码而已，但是卡在哪里，她花了多少钱，花在哪里了，有没有借钱给别人，我通通不知道，这让我很不安。况且她没有理财观念，我曾经三番五次跟她说了理财的重要性，她就只是沉默不语，也不知道是听不懂，还是装傻实际上自己去做了理财但是没把收入情况告诉我，我们多次吵架都是为了钱，我觉得夫妻两个在这个问题上不能坦诚是个大问题，我现在很想经济分开，但是不知道怎么做才比较妥当'\n",
    "    ),\n",
    "    (\n",
    "        '30岁男生，相亲屡次不顺，我该怎么办呢？',\n",
    "        '30岁男生还单身，相亲对象不少，可是次次都以失败收场，自己长相还行，就是不太会说话，和女生聊天刚开始聊的挺不错，没聊几天女的就不回复了，爱答不理，心里郁闷的晚上都想哭，我该怎么办'\n",
    "    ),\n",
    "    (\n",
    "        '30岁女生，离婚念头挥之不去，他为何计划婚内出轨？',\n",
    "        '此刻有点儿失眠心情烦躁，我想了很久没有想明白，为什么他会计划着婚内出轨，……心里，我基本上认定原因应该是我身材不怎么好，胸部不够吸引人，因为婚后发现了在婚前，他约P了一个又肥又长相不佳的女人。我以为就是因为对方胸部傲人。在壹心理一些文章的引导下，我突然明白，源头是性生活不和谐。婚前同居一年多，到婚后2年，性生活基本就是，直奔主题，他一手拿着手机欣赏着他的成人影片一边爱爱，很少会主动关心我的感受，也很少理会我让他收起手机的要求。而且另一方面，他基本每晚都会要求我用嘴亲亲，他自己则拿着手机看片或者无关紧要的东西，让我经常觉得自己就是一个娃娃一个工具。以前我主动跟他说起过几次，然而也只是不了了之。后来我基本对性生活没有兴趣，也很反感为他口J。而且，日常生活变得频繁小吵冷战，很少有之前无话不谈的亲密，我不知道我们还有没有重归于好的机会……'\n",
    "    ),\n",
    "    (\n",
    "        '29岁哺乳期，和公婆因带孩子问题每天崩溃，怎么办？',\n",
    "        '怀孕她就没怎么管，备产各种东西都是自己准备。产后直接是妈妈照顾的。产后半年回来要上班，我爸妈又没退休，无奈下还是他们来照顾。他们方法不对，连个衣服都洗不干净，更不用提孩子的饭了。每天我都自己趁下班时间给孩子准备饭和各种东西。最怕的是习惯，比如她喜欢喂孩子各种炒菜，我说好几次一岁之内吃盐不好就是不听，还故意喂。或者孩子的奶瓶，经常不盖盖子让他把玩，说不卫生，也不搭理。孩子吃饭的碗和勺子永远是扔在各种桌子椅子角落，用的时候直接拿起来用也不洗，洗脸洗脚水每次告诉温度差不多就好了，就是不听，偏要一会冷一会烫。来我家被子也不叠，锅永远是不会洗的，下一顿加水加米接着煮。我老公又每天忙的不在家，我已经要崩溃了'\n",
    "    ),\n",
    "    (\n",
    "        '婚后分居三四年，有过家庭矛盾，我的婚姻是否该继续？',\n",
    "        '婚后分居三四年，和老公之间发生过一些家庭矛盾，可以看我另外一篇疑问。我们上半年闹离婚，后来他又主动和好，但是我们之前除了新婚，几乎很少有夫妻生活，他说一靠近我就觉得心里像一堆蚂蚁抓挠，特别难受。他说这是他得心理问题，小时候受到唾骂和凌辱，造成了心理阴影。我们俩都觉得生活的很痛苦，但为了孩子没有离婚。我跟他在一起就觉得拘束，不自在。向往的温馨家庭生活也得不到，我很累，我应该坚持离婚吗？他这种心理疾病是真的还是借口呢？'\n",
    "    ),\n",
    "#     (\n",
    "#         '',\n",
    "#         '新 闻 大S自嘲上辈子应是儿子的“杀父仇人”，笑说：“不然我怎么一生他就快死了，他是来报仇的吧，而且我一抱他就哭，到三个月后才好一点。”她生下一双儿女集满“好”字，感到非常满足，是否再拼第三胎？大S笑说：“不可能！我现在尽量和老公分房睡。” 评析起初，大家并不看好大S与汪小菲这段婚姻。姐弟恋是一回事儿，更多还是觉得有些不够门当户对。如今，女儿与儿子陆续出生。凑足“好”字的两人，幸福得就像花儿开放。俏江南的变化，对大S与汪小菲毫无影响。男的负责赚钱养家，女的负责照顾孩子开心美满。'\n",
    "#     )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.recompute=True\n",
    "# args.top_p=0.0\n",
    "# args.top_k=0\n",
    "# args.temperature=1.0\n",
    "\n",
    "n_gen = 2\n",
    "\n",
    "for title, text in input_texts:\n",
    "    txt = f'{title}<sep>{text}<|endoftext|>'\n",
    "    context_tokens = tokenizer.EncodeAsIds(txt).tokenization\n",
    "    print(title)\n",
    "    print(text)\n",
    "    print()\n",
    "    for i in range(n_gen):\n",
    "#         args.temperature=random.gauss(0.95, 0.05)\n",
    "        print(f'{i}: ', end='')\n",
    "        s_pred = ''\n",
    "        for s in infer_text_generative(txt, model, tokenizer):\n",
    "            print(s, end='')\n",
    "        print(os.linesep)\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动续写测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "'''\n",
    "婚后分居三四年，有过家庭矛盾，我的婚姻是否该继续？\n",
    "''',\n",
    "'''\n",
    "婚后分居三四年，和老公之间发生过一些家庭矛盾，可以看我另外一篇疑问。\n",
    "我们上半年闹离婚，后来他又主动和好，但是我们之前除了新婚，几乎很少有夫妻生活，他说一靠近我就觉得心里像一堆蚂蚁抓挠，特别难受。他说这是他得心理问题，小时候受到唾骂和凌辱，造成了心理阴影。\n",
    "我们俩都觉得生活的很痛苦，但为了孩子没有离婚。我跟他在一起就觉得拘束，不自在。\n",
    "向往的温馨家庭生活也得不到，我很累，我应该坚持离婚吗？\n",
    "他这种心理疾病是真的还是借口呢？\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这在里写我的回答**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里写一部分\n",
    "answer = '''\n",
    "我觉得这是你\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**执行下面的 cell ，帮我写把上面这句写完，或者写下一句**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pred = 3\n",
    "\n",
    "for i in range(n_pred):\n",
    "    \n",
    "    txt = '<sep>'.join(s.strip() for s in question) + '<|endoftext|>' + '<bos>' + answer.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(txt).tokenization\n",
    "    \n",
    "    print(f'{i+1}: ', end='')\n",
    "    s_pred = ''\n",
    "    with closing(\n",
    "        infer_text_generative(txt, model, tokenizer)\n",
    "    ) as generator:\n",
    "        for s in generator:\n",
    "            if s:\n",
    "                print(s, end='')\n",
    "                if s[-1] in ('.。!！?？'):\n",
    "                    break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
