{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron 直接使用预训练模型进行预测 (BPE v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "准备运行这个笔记本的 Jupyter kernel(**如果已经准备就绪，不要重复执行！**)：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 配置一个 Conda 环境作为 Jupyter Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda env update -f environments/environment-ipy.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装完毕后，为该 Notebook 选择这个 Kernel (名为`Megatron_LM-ipy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在Kernel所在 Conda 环境中安装 Apex\n",
    "\n",
    "需要通过 pip 从 github 下载源代码安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -v -r requirements/apex.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD\n",
    "\n",
    "定位到工作目录，根据具体情况决定哦，不一定是下面的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 指定 Checkpoints 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从 S3 下载\n",
    "\n",
    "文件比较大，根据实际情况选择下载，**不要重复下载**\n",
    "\n",
    "如果需要下载，修改路径等，然后执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/latest_checkpointed_iteration.txt to checkpoints/345m-hmwebmix-bpe-v2/latest_checkpointed_iteration.txt\n",
      "s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/iter_0390000 ==> ./checkpoints/345m-hmwebmix-bpe-v2/iter_0390000\n",
      "load:  ./checkpoints/345m-hmwebmix-bpe-v2\n",
      "iteration:  390000\n",
      "CPU times: user 55.7 ms, sys: 20.1 ms, total: 75.8 ms\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "HPARAMS_NAME = '345m'\n",
    "MODEL_NAME = '345m-hmwebmix-bpe-v2' # '345m-hmwebmix-bpe-v2' # '345m-xinliqa-hunyin' # '117m-hmwebmix_191128-bpe_v2' #  \n",
    "TOKENIZER_TYPE = 'GPT2BPETokenizer_CN'\n",
    "\n",
    "AWSS3_CKPTS_DIR = os.path.join('s3://huamei/hmgpt2-checkpoints', MODEL_NAME)\n",
    "LOCAL_CKPTS_DIR = os.path.join('./checkpoints', MODEL_NAME)\n",
    "\n",
    "# 复制 latest_checkpointed_iteration.txt\n",
    "!aws s3 cp \\\n",
    "    {AWSS3_CKPTS_DIR} \\\n",
    "    {LOCAL_CKPTS_DIR} \\\n",
    "    --recursive \\\n",
    "    --exclude \"*\" \\\n",
    "    --include \"latest_checkpointed_iteration.txt\"\n",
    "\n",
    "# 下载后读取最新的 checkpoint iter 名称\n",
    "iteration = open(f'{LOCAL_CKPTS_DIR}/latest_checkpointed_iteration.txt').read()\n",
    "iteration = int(iteration)\n",
    "iteration_dir = 'iter_{:07d}'.format(iteration)\n",
    "\n",
    "awss3_ckpt_dir = os.path.join(AWSS3_CKPTS_DIR, iteration_dir)\n",
    "local_ckpt_dir = os.path.join(LOCAL_CKPTS_DIR, iteration_dir)\n",
    "\n",
    "print(f'{awss3_ckpt_dir} ==> {local_ckpt_dir}')\n",
    "    \n",
    "# 同步最新的 Checkpiont\n",
    "!aws s3 sync {awss3_ckpt_dir} {local_ckpt_dir}\n",
    "\n",
    "#\n",
    "load_model_dir = LOCAL_CKPTS_DIR\n",
    "print('load: ', load_model_dir)\n",
    "print('iteration: ', iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接使用本地\n",
    "\n",
    "- 另外一个选择：直接使用本地的已有模型\n",
    "\n",
    "修改超参数, 路径等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load:  ./checkpoints/117m.spm-xinli_qa-hunyin\n",
      "iteration:  60000\n",
      "CPU times: user 1.92 ms, sys: 350 µs, total: 2.26 ms\n",
      "Wall time: 9.16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "HPARAMS_NAME = '117m'\n",
    "MODEL_NAME = '117m.spm-xinli_qa-hunyin'\n",
    "TOKENIZER_TYPE = 'SentencePieceTokenizer'\n",
    "\n",
    "AWSS3_CKPTS_DIR = os.path.join('s3://huamei/hmgpt2-checkpoints', MODEL_NAME)\n",
    "LOCAL_CKPTS_DIR = os.path.join('./checkpoints', MODEL_NAME)\n",
    "\n",
    "\n",
    "# 读取最新的 checkpoint iter 名称\n",
    "iteration = open(f'{LOCAL_CKPTS_DIR}/latest_checkpointed_iteration.txt').read()\n",
    "iteration_dir = 'iter_{:07d}'.format(int(iteration))\n",
    "\n",
    "local_ckpt_dir = os.path.join(LOCAL_CKPTS_DIR, iteration_dir)\n",
    "\n",
    "load_model_dir = LOCAL_CKPTS_DIR\n",
    "print('load: ', load_model_dir)\n",
    "print('iteration: ', iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用哪个/些 GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import copy, deepcopy\n",
    "from contextlib import closing\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import chain, compress\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from pretrain_gpt2 import initialize_distributed, get_masks_and_position_ids, set_random_seed\n",
    "from generate_samples import prepare_tokenizer, setup_model, get_token_stream\n",
    "# from predict_gpt2 import initialize_distributed, prepare_tokenizer, set_random_seed, setup_model, get_token_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    # To be updated ...\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=1024,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "    tokenizer_type=TOKENIZER_TYPE,\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    cache_dir=None,\n",
    "    # Training arguments.\n",
    "    load=load_model_dir,\n",
    "    seed=1234,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    resume_dataloader=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None, \n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "    out_seq_length=128,\n",
    ")\n",
    "\n",
    "\n",
    "# restore generating args\n",
    "def reset_generating_args(args):\n",
    "    args.recompute=False\n",
    "    args.top_p=0.0\n",
    "    args.top_k=0\n",
    "    args.temperature=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS_SCHEMA = {\n",
    "    '117m': dict(\n",
    "        num_layers=12,\n",
    "        hidden_size=768,\n",
    "        num_attention_heads=12,\n",
    "        max_position_embeddings=1024,\n",
    "    ),\n",
    "    '345m': dict(\n",
    "        num_layers=24,\n",
    "        hidden_size=1024,\n",
    "        num_attention_heads=16,\n",
    "        max_position_embeddings=1024,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 设置 GPT-2 模型的超参数\n",
    "for k, v in HPARAMS_SCHEMA[HPARAMS_NAME].items():\n",
    "    setattr(args, k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(DDP_impl='local', attention_dropout=0.1, cache_dir=None, checkpoint_activations=None, checkpoint_num_layers=1, cuda=True, distributed_backend='nccl', dynamic_loss_scale=True, eod_mask_loss=None, finetune=None, fp16=True, greedy=False, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, load='./checkpoints/345m-hmwebmix-bpe-v2', local_rank=None, loss_scale=None, loss_scale_window=1000, make_vocab_size_divisible_by=128, max_position_embeddings=1024, min_scale=1, model_parallel_size=1, no_load_optim=None, no_load_rng=None, num_attention_heads=16, num_layers=24, out_seq_length=128, rank=0, recompute=None, reset_attention_mask=None, reset_position_ids=None, resume_dataloader=None, seed=1234, seq_length=1024, temperature=1.0, tokenizer_model_type='bert-large-uncased', tokenizer_path='./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model', tokenizer_type='GPT2BPETokenizer_CN', top_k=0, top_p=0.0, vocab_size=None, world_size=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化函数/全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def initialize():\n",
    "    global model, tokenizer\n",
    "\n",
    "    # Disable CuDNN.\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Pytorch distributed.\n",
    "    initialize_distributed(args)\n",
    "\n",
    "    # Random seeds for reproducability.\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    # get the tokenizer\n",
    "    tokenizer = prepare_tokenizer(args)\n",
    "\n",
    "    # Model, optimizer, and learning rate.\n",
    "    model = setup_model(args)\n",
    "\n",
    "    args.device = torch.cuda.current_device()\n",
    "\n",
    "    # setting default batch size to 1\n",
    "    args.batch_size = 1\n",
    "\n",
    "    assert mpu.get_model_parallel_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主进程初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "prepare tokenizer done\n",
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 0: 336128000\n",
      "global rank 0 is loading checkpoint ./checkpoints/345m-hmwebmix-bpe-v2/iter_0390000/mp_rank_00/model_optim_rng.pt\n",
      "  successfully loaded ./checkpoints/345m-hmwebmix-bpe-v2/iter_0390000/mp_rank_00/model_optim_rng.pt\n",
      "CPU times: user 11.6 s, sys: 4.4 s, total: 16 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tokens_generative(context_tokens, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        context_length = len(context_tokens)\n",
    "        token_stream = get_token_stream(model, [context_tokens], tokenizer, args)   \n",
    "        for i, (output_tokens, _) in enumerate(token_stream):\n",
    "            if context_length + i >= args.seq_length:\n",
    "                break\n",
    "            ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "            yield ids[-1]\n",
    "\n",
    "\n",
    "def infer_text_generative(contex_text, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        contex_text = contex_text.strip()\n",
    "        context_tokens = tokenizer.EncodeAsIds(contex_text).tokenization\n",
    "        context_length = len(context_tokens)\n",
    "\n",
    "        token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "\n",
    "        for i, (output_tokens, _) in enumerate(token_stream):\n",
    "            if context_length + i >= args.seq_length:\n",
    "                break\n",
    "            ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "            s = tokenizer.DecodeIds([ids[-1]])\n",
    "            yield s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测试试看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想和男人谈钱不伤感情，前提只有一个，做到了钱和感情就都有了 \n",
      "==========\n",
      "\n",
      "1/3:\t。现在扯到谈感情,就硬扯到钱,让女人退回她的地盘,让男人谈钱后悔。我实在不知道这波操作算不算淫荡,所有的文字里,你绝对想不到自己是深爱这个女人的,她和你谈钱,是对你的爱吗?男人要有钱,至少50万。你要是甩女人一个大嘴巴子,脸皮厚到真能连脸带鼻子来复仇了,说不定你老婆会对你另娶此女,因为她知道你还剩5万年薪;如果你不要脸,哪留着呢,到时有3个人抢着要,\n",
      "\n",
      "2/3:\t。大蛇丸信笔生花,幅度之大,自不潇洒:既然不想不懂感谢,算了,就把我列入你毛条钱的范畴内吧:“首先,在被告知获得官晶之后,才能拿发票到日本,这是为了让日本方面将发票送回我的单位。”这一点,句句交代,事情的来龙去脉都清楚了。即便是有笔者经验丰富的“女秘书”栗原率先撕闪了,“我还是想亲口问问,一拍卖前,吉本老师会让一亿只贵宾室里摆满贵宾席,吉本\n",
      "\n",
      "3/3:\t。在谈资金周转的时候不得注意揣摩着眼前的小钱。别把你的钱象影片中的仆人一样让男人养,区别是嫁入豪门不等于嫁入了国,而爱情比钱更重要。除非你要求钱,那就请明确地告诉他:如今的票子,重要到能与生命的价值相比,甚至是生命的容颜。好了,爱情和物质,在某些男人眼里过过嘴瘾来,过过嘴瘾而已样资不一定要丰盛,精神上富足不一定是车房,只是罗纳德所向往的,年轻的时候向往着一切\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contex_text = '想和男人谈钱不伤感情，前提只有一个，做到了钱和感情就都有了 '\n",
    "# '一个疯子进去学校把凳子打烂了，保安不敢动手，报警了，警察现在来了'\n",
    "# '我喜欢'\n",
    "\n",
    "print(contex_text)\n",
    "print('==========')\n",
    "print()\n",
    "\n",
    "N = 3\n",
    "for i in range(N):\n",
    "    print(f'{1+i}/{N}:\\t', end='')\n",
    "    for s in infer_text_generative(contex_text.strip(), model, tokenizer):\n",
    "        print(s, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 test 语料，从中随机打断，并预测下文，比较原文与预测结果！\n",
    "\n",
    "随机选 N 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_file=./data/hmwebmix_191203/predict.191205170349-345m.iter_0390000-100x3x128-r1s1.tsv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "ts = datetime.now().strftime('%y%m%d%H%M%S')\n",
    "\n",
    "N = 100\n",
    "C = 3\n",
    "sample_random = 1\n",
    "sample_shuffle = 1\n",
    "\n",
    "input_file = './data/hmwebmix_191203/test.json'\n",
    "output_file = f'./data/hmwebmix_191203/predict.{ts}-{HPARAMS_NAME}.{iteration_dir}-{N}x{C}x{args.out_seq_length}-r{sample_random}s{sample_shuffle}.tsv'\n",
    "\n",
    "print(f'output_file={output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61e386e972e41d087a47d6b259b865a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 数据总数: 18,894\n",
      "Test 采样数: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c1be917a7a4c7c81620a24e7b6979c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='sample', style=ProgressStyle(description_width='initial')), H…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702e7ae562ff4be1a57f5acafb13e1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='infer', style=ProgressStyle(description_width='initial')), HT…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = sum(1 for _ in tqdm(open(input_file)))\n",
    "print(f'Test 数据总数: {total:,d}')\n",
    "\n",
    "assert total >= N\n",
    "\n",
    "print(f'Test 采样数: {N:,d}')\n",
    "\n",
    "mask = np.zeros(total, dtype=int)\n",
    "mask[:N] = 1\n",
    "if sample_random:\n",
    "    np.random.shuffle(mask)\n",
    "\n",
    "samples = []\n",
    "with open(input_file) as fp:\n",
    "    reader = compress(fp, mask)\n",
    "    for line in tqdm(reader, 'sample', total=N):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        text = json.loads(line)['text']\n",
    "        samples.append(text)\n",
    "\n",
    "if sample_shuffle:\n",
    "    random.shuffle(samples)\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    writer = csv.writer(fp, delimiter='\\t')\n",
    "    for context_txt in tqdm(samples, 'infer'):\n",
    "        context_ids = tokenizer.EncodeAsIds(context_txt).tokenization\n",
    "        context_len = len(context_ids)\n",
    "        idx = round(random.gauss(context_len*0.5, context_len*0.1))\n",
    "        input_ids = context_ids[:idx]\n",
    "        label_ids = context_ids[idx:]\n",
    "        row = [\n",
    "            tokenizer.DecodeIds(ids)\n",
    "            for ids in (input_ids, label_ids)\n",
    "        ]\n",
    "        for _ in range(C):\n",
    "            reset_generating_args(args)\n",
    "            try:\n",
    "                args.recompute=True\n",
    "                args.top_p=random.gauss(0.5, 0.5)\n",
    "                args.temperature=random.gauss(1, 0.05)\n",
    "                infer_ids = list(infer_tokens_generative(copy(input_ids), model, tokenizer))\n",
    "                row.append(tokenizer.DecodeIds(infer_ids))\n",
    "            finally:\n",
    "                reset_generating_args(args)\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
