{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron XinLi QA 预测 (BPE v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "准备运行这个笔记本的 Jupyter kernel(**如果已经准备就绪，不要重复执行！**)：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 配置一个 Conda 环境作为 Jupyter Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda env update -f environments/environment-ipy.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装完毕后，为该 Notebook 选择这个 Kernel (名为`Megatron_LM-ipy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在Kernel所在 Conda 环境中安装 Apex\n",
    "\n",
    "需要通过 pip 从 github 下载源代码安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -v -r requirements/apex.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD\n",
    "\n",
    "定位到工作目录，根据具体情况决定哦，不一定是下面的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 Checkpoints\n",
    "\n",
    "文件比较大，根据实际情况选择下载，**不要重复下载**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/iter_0190000/mp_rank_00/model_optim_rng.pt to checkpoints/345m-hmwebmix-bpe-v2/iter_0190000/mp_rank_00/model_optim_rng.pt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/iter_0190000 ./checkpoints/345m-hmwebmix-bpe-v2/iter_0190000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/latest_checkpointed_iteration.txt to checkpoints/345m-hmwebmix-bpe-v2/latest_checkpointed_iteration.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://huamei/hmgpt2-checkpoints/345m-hmwebmix-bpe-v2/latest_checkpointed_iteration.txt ./checkpoints/345m-hmwebmix-bpe-v2/latest_checkpointed_iteration.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用哪个/些 GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "from itertools import chain, compress\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from pretrain_gpt2 import get_masks_and_position_ids\n",
    "from predict_gpt2 import initialize_distributed, prepare_tokenizer, set_random_seed, setup_model, get_token_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    num_layers=24,\n",
    "    hidden_size=1024,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=1024,\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=1024,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "    tokenizer_type='GPT2BPETokenizer_CN',\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    cache_dir=None,\n",
    "    # Training arguments.\n",
    "    load='./checkpoints/345m-hmwebmix-bpe-v2/',\n",
    "    seed=1234,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    resume_dataloader=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None, \n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "    out_seq_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化函数/全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def initialize():\n",
    "    global model, tokenizer\n",
    "\n",
    "    # Disable CuDNN.\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Pytorch distributed.\n",
    "    initialize_distributed(args)\n",
    "\n",
    "    # Random seeds for reproducability.\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    # get the tokenizer\n",
    "    tokenizer = prepare_tokenizer(args)\n",
    "\n",
    "    # Model, optimizer, and learning rate.\n",
    "    model = setup_model(args)\n",
    "\n",
    "    args.device = torch.cuda.current_device()\n",
    "\n",
    "    # setting default batch size to 1\n",
    "    args.batch_size = 1\n",
    "\n",
    "    assert mpu.get_model_parallel_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主进程初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "prepare tokenizer done\n",
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 0: 336128000\n",
      "global rank 0 is loading checkpoint ./checkpoints/345m-hmwebmix-bpe-v2/iter_0190000/mp_rank_00/model_optim_rng.pt\n",
      "  successfully loaded ./checkpoints/345m-hmwebmix-bpe-v2/iter_0190000/mp_rank_00/model_optim_rng.pt\n",
      "CPU times: user 11.4 s, sys: 3.83 s, total: 15.2 s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tokens_generative(context_tokens, model, tokenizer):\n",
    "    context_length = len(context_tokens)\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)   \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        yield ids[-1]\n",
    "\n",
    "\n",
    "def infer_text_generative(contex_text, model, tokenizer):\n",
    "    contex_text = contex_text.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(contex_text).tokenization\n",
    "    context_length = len(context_tokens)\n",
    "\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "    \n",
    "    for i, (output_tokens, _) in enumerate(token_stream):\n",
    "        if context_length + i >= args.seq_length:\n",
    "            break\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        s = tokenizer.DecodeIds([ids[-1]])\n",
    "        yield s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测试试看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我喜欢\n",
      "==========\n",
      "\n",
      "(1)/3\t水稻田里的那句话:“种瓜得豆,种豆得瓜,日长一粒长一粒。”不愿意承认长粒需要天,更不愿“踏破铁鞋无觅处,得来全不费功夫。”永远明白自己能干什么,可以干什么的人,永远不会失去个性,永远不会消沉,会一直进步着。是一个没有与自己厮守的人,不会感到寂寞,在寂寞中还能设身处地的走入别人的漩涡深处的存在,把注意力更多的专注于帮助别人。与此相反,另一种人始终喜欢创造无限的可能,即使是最不起眼的,甚至简单的黑暗,也要让自己闪烁的出奇。“舍得”是我们中国人俗话的表述。我们很乐见这样的场景或实际发生的事来:一个人孤身一个人来这里种田,午间的阳光照在他身上,白而没有光芒,仿佛是遍地凋零的叶子,顺着风走,狼狈逃窜,灵魂跟着她越是投奔黑暗的地方,越是显得恍惚。这时候,他旁边墙角上的一棵杂草开始盛开着,边上有一只狗,悄悄的爬了过来,嗅着人的气息,怎么刚刚,这里只有这个人一个人在种田\n",
      "\n",
      "(2)/3\t游览这火车之旅,火车之旅有点像我抱着一沓厚厚的履历书,从东部到西部,一点一滴发生的故事。Swohr敢于提闪亮的名字,也许会让湮没在光芒中的我信心满满。梭罗湖不仅是野生动植物的神奇之地,也是摄影大师王家卫的作品高峰地。这是渡过了一切,世与喧嚣、俗与庸碌,尘世的喧嚣在夜幕中复苏,桥底的小渔船独上风月楼。这一切都来不及在意,可是自己却难以抗拒。在火车上,你能十分真切地看到火车,200一米宽的屋顶莫非能拍出2000cm以上的高速高崎透视吗?Sohr不只是闪耀着法国蓬勃的工业发展,成群的猪狗和Tuesday标签模特,才是它最大的标签。每次一节车厢的人,男人大多清瘦,女人细很多像中国新疆少数民族少女,或五十几岁的形象、干脆鲁莽,来去匆匆,觉察不到怜爱和爱惜。也许她们都很美、但仍清楚地做着坐着的优雅,或说着就\n",
      "\n",
      "(3)/3\t温暖的感觉,喜欢细雨滋润的空间,喜欢聚集在一起打闹的感觉。特别是雨天,听拿捏的焦透的雨声,心情舒畅鸟自在天。下雨天,仿佛是我的任何朋友。要说怎样才会有一个最可爱但是又很自私的朋友,出外面玩和吃饭唯一的一个快乐。在屋顶上,看到雨了。而我想意即我之下,这就好像到了户外呢。越下雨,狂风越大,一派大雨如注漫天的来了。小鸟在飞,不论不用伞的,都一下子飞了出去。哇趴,我会唱歌给你听,这个地方只有我们两个人。嘿嘿,是不是很富日记了。下雨天,我第一个想到的是雨。下雨天,雨水会更肆意的打在身上,我想我想要透了。上一次这样是下雨天。做了手工整理,漏了放大图,那是我的独门绝技,就是将北京给我的上古先祖的玉龙抛下了山。北京到底是谁上山祈雨。是不是也留下来呢,我想还留着呢。桃花开的一霎刹那,我就看到了一九九O年牡丹盛放的那一刻。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contex_text = '我喜欢'\n",
    "\n",
    "print(contex_text)\n",
    "print('==========')\n",
    "print()\n",
    "\n",
    "N = 3\n",
    "for i in range(N):\n",
    "    print(f'({1+i})/{N}\\t', end='')\n",
    "    for s in infer_text_generative(contex_text.strip(), model, tokenizer):\n",
    "        print(s, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对一组心理/情绪相关文字进行续写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    '宝贝“啃指甲”是为什么呢？宝贝是缺铁、缺锌了吗？宝贝是肚子里有虫吗？这指甲里好多细菌宝宝都吃下去怎么是好？怎么能让宝宝不“啃指甲”呢？家长应该怎么应对呢？',\n",
    "    '几年前，英国就有专家向国会递交报告，建议政府制定法律禁止家长让年龄低于3岁的儿童看电视。奇幻小说《查理和巧克力工厂》有句台词：“千万、千万、千万别让孩子，靠近你的电视，最好是别购买、安装，这最最愚蠢的东西”。',\n",
    "    '婚姻是一座炼狱，因为在这场婚姻里夹杂着各种人际关系，经过这些人际关系，人才能慢慢心智成熟起来。而能够把这些人际关系处理好的，往往都是生活中的高手。那么夫妻双方该如何经营婚姻呢？这里就提供一些方法供参考。',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宝贝“啃指甲”是为什么呢？宝贝是缺铁、缺锌了吗？宝贝是肚子里有虫吗？这指甲里好多细菌宝宝都吃下去怎么是好？怎么能让宝宝不“啃指甲”呢？家长应该怎么应对呢？\n",
      "\n",
      "0: 我们知道婴幼儿时期的宝宝比较喜欢吮吸大人手指的奶嘴或者手指的玩具,特别是玩具的碎末,如果宝宝经常的大口或过快的吮吸和吮吸手指而且常常还会吮到头发上,就表明该幼儿可能存在肠胃疾病或者有碍健康的可能性。如果非常频繁的长指甲会导致指甲短小变厚或者揉捏不自主,甚至会啃掉手指甚至指甲根部位,影响手指的正常形状和形状的保持和指甲的生长。重要的是当宝宝吃手指的时候就不能一直吮吸指甲,应喂小些。而且,孩子的指甲长得粗大和弯曲的时候可能是在尝试某种新的玩具。总之,宝宝的指甲以小指尖长、圆形为好,如果指甲呈尖尖的形状就不行。在我小的时候,我也啃指甲过的,因为喜欢,特别让人痛心,我还屡试不爽。孩子还会啃指甲,就是如果孩子指头粘在大人的手背上或者盘子里,不妨就用指甲刀把孩子的指甲割下来,平时也不要让孩子接触任何器皿。如果是自己的吃的,建议给宝宝多喝水吃水果,多晒太阳,多和爸爸妈妈在家运动为好。带孩子去查\n",
      "\n",
      "1: 历史原因艾滋病病毒可以在粘土中生存,当温度降低时生命便会结束。沿海地区的婴儿,忽然生病或医学上的疾病,比其他地区婴儿得病更快,死亡率低。目前在你的各儿子手腕上各用手腕毛细血管。3.伤口艾滋病:吞噬伤口中的炮球菌,对病情十分严重,可造成真正的重症。许多婴儿死亡的内因是:多喝水。许多婴儿在婴儿期咳嗽或发烧时,经常咳出很黏稠的皮疹。后来发现这些东西里有“琴弦”在转移。治疗方法:立即将孩子病人的伤口涂擦“琴弦”溶液。洗内裤时,避免使用泻药。患儿眼睛很易于黏上清洁的手纸,一滴清水点入便可消失。目前在你的各儿子手腕上各用手腕毛细血管。药理治疗肌注可显著降低患儿脑电图反应。很小的婴儿在脐带缠成的管内会发生塌陷性呼吸,几分钟至一周左右就会完全停止。约有82%的病例之抑制该现象有效。预防措施:遵守医院的操作规程,\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "几年前，英国就有专家向国会递交报告，建议政府制定法律禁止家长让年龄低于3岁的儿童看电视。奇幻小说《查理和巧克力工厂》有句台词：“千万、千万、千万别让孩子，靠近你的电视，最好是别购买、安装，这最最愚蠢的东西”。\n",
      "\n",
      "0: 但遗憾的是,家长们直到今天才幡然醒悟。孩子视力下降,跟电视在生活中的作用没多大关系。向孩子喜欢的电视正式购买、安装都没有任何问题。那为什么还要让小孩不看电视呢?加拿大政府办公室指出,每台电视接收到的信号占10兆到20兆像素,而儿童是一个障碍。所以,在现实生活中,电视的激烈显示数字使看电视的概率明显降低。1948年,第一代DVD“液晶”机在法国首次亮相,在科学技术极大进步的今天,普通电视节目已经没有多少吸引力了,人们更愿意观看数字搜索服务。用“调查显示”实证数据分析,目前几乎所有数字图像产品能够解决的真实度仅有1%至1.5%。迄今,在全世界范围内,3.3%的成年人每天凌晨视力持续平稳下降,30%~40%,0.5%至1%的成年人视力下降,超过20%。与此相比,法语、汉语、亚洲、拉丁语、及非洲土著语都在缓步下降。维持日常活动需要你进行大量工作,眼睛得到更多空间消耗。由于看不到真实图像,我们无法想象会发生\n",
      "\n",
      "1: 话虽荒唐,但的确反应出了家长控制视频播放、轻易登上电视影响孩子日常生活的无奈。虽然现在有法防止未成年人观看,但谁家孩子能不听电视呢?11月3日,河南省有一名家长在小区卖车,涨价34%。然而,出乎意料的是,小瞧孩子的年龄并不比父母想象的小,两三岁的小孩甚至可以跟大人推着手推车进进出出。小男孩虽看电视,但恰恰是他爸妈二人轮流看,就好像两人轮流超过某一个年龄段,这究竟是好的现象?还是坏的现象?其实,除了小孩,很多成年人也会看电视。无关年龄,全看你收视率。北美电视台在商业节目中统计了电视观众的年龄分布。在56至59岁之间的人群中,收看电视的人数明显多于收看其他娱乐节目的人。收看电视掩盖了孤独、情感的各种问题,电视知道儿童用情感共鸣解决冲突、家长观察儿童的反应,一些节目更能使得孩子染上网瘾甚至家暴。以前也有人说,小孩子就看电视,等长大父母也变老了,这些节目自然也是要看。日本的教育\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "婚姻是一座炼狱，因为在这场婚姻里夹杂着各种人际关系，经过这些人际关系，人才能慢慢心智成熟起来。而能够把这些人际关系处理好的，往往都是生活中的高手。那么夫妻双方该如何经营婚姻呢？这里就提供一些方法供参考。\n",
      "\n",
      "0: 第一,坐下来好好吃饭,晚上好好聊天。夫妻两人如果想把婚姻经营好,那么一定要做到这一点。尤其是在婚姻中彼此都想把对方凸显出来,不想对方总是绷着脸,或无精打采的。当然,这是前提。这就与开始的时候不一样了,第二,改变自己的穿着,改掉自己的一身小毛病。只有让自己完全顾自己的心情,任班不累,全心全意陪在对方身上的时间减少了,这样才不会给自己留下心有芥蒂的余地。比如说,在家里,你家的女人总是按照你的意思来安排,不吃水果不洗碗,家里的大扫除从不拖地,而外面是男人的风卷残云。有人就会问你不能在家里放下辣椒洗个脸拖地。面对这样的女人,你想到了要去改变,你要想着如何把女人的话套进你的口,并且在她的异样中心里找到属于她能听懂的借口。当然,男人还是得自己靠谱。有些话被女人听了是要生气的,不悦的,不可擅自发挥的,所以,努力去让自己变得踏实总是可以的。比如说,这是我的经验,平常你如果不仔细看,根本不会发现,哪里粗哪些细。\n",
      "\n",
      "1: 夫妻如同狼族,其生存的特性便是:人心隔肚皮、人与人之间彼此猜忌、语言不通畅、认知障碍和信息不对称。而处理好这层关系,可以让彼此的关系融洽起来,多做好事情总是好事。记得从小的时候,爸爸妈妈把我们兄妹四个叫到自己办公室里来,大概已跨越了几代人的时光。大人们之间围绕孩子之间的矛盾、婚姻中的分歧,几乎统统都能通过交谈而得到解决。我们年纪小的时候,没和我们年龄相仿的小朋友一起玩,比如说去年的经历、下年的打算等,我们被要求非常容易被父母分析和分析来分析去,获得完全的了解。往往这些事情外人一看都是需要被强迫而逐步向我们传达的信息,而我看出来他们是有目的有计划地去干那些事情,当时觉得有些不可理喻。我在初中的时候,虽有爸妈的陪伴,也尽情与其它小朋友做好玩的事情,但那些时间一过,又变得没有什么存在感,我的存在感也就随之消失殆尽,我的真实存在的价值与集体来的效果便为零。我们相处的少,常常会觉得愧疚与不安。别以为这个社会不管\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.recompute=True\n",
    "# args.top_p=0.0\n",
    "# args.top_k=0\n",
    "# args.temperature=1.0\n",
    "\n",
    "n_gen = 2\n",
    "\n",
    "for txt in input_texts:\n",
    "    context_tokens = tokenizer.EncodeAsIds(txt).tokenization\n",
    "    print(txt)\n",
    "    print()\n",
    "    for i in range(n_gen):\n",
    "#         args.temperature=random.gauss(0.95, 0.05)\n",
    "        print(f'{i}: ', end='')\n",
    "        s_pred = ''\n",
    "        for s in infer_text_generative(txt, model, tokenizer):\n",
    "            print(s, end='')\n",
    "        print(os.linesep)\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 test 语料，从中随机打断，并预测下文，比较原文与预测结果！\n",
    "\n",
    "随机选 N 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 107360.44it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 数据总数: 10,000\n",
      "Test 采样数: 1,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:04:12<00:00,  3.85s/it]\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "\n",
    "input_file = './data/hmwebmix/hmwebmix.test.json'\n",
    "output_file = f'./data/hmwebmix/hmwebmix.test.{args.out_seq_length}.{N}.tsv'\n",
    "\n",
    "total = sum(1 for _ in tqdm(open(input_file)))\n",
    "print(f'Test 数据总数: {total:,d}')\n",
    "\n",
    "assert total >= N\n",
    "\n",
    "print(f'Test 采样数: {N:,d}')\n",
    "\n",
    "mask = np.zeros(total, dtype=int)\n",
    "mask[:N] = 1\n",
    "np.random.shuffle(mask)\n",
    "\n",
    "with open(input_file) as input_fp, \\\n",
    "     open(output_file, 'w') as output_fp:\n",
    "    reader = compress(input_fp, mask)\n",
    "    writer = csv.writer(output_fp, delimiter='\\t')\n",
    "    for line in tqdm(reader, total=N):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        context_txt = json.loads(line)['text']\n",
    "        context_ids = tokenizer.EncodeAsIds(context_txt).tokenization\n",
    "        context_len = len(context_ids)\n",
    "        idx = round(random.gauss(context_len*0.5, context_len*0.1))\n",
    "        input_ids = context_ids[:idx]\n",
    "        label_ids = context_ids[idx:]\n",
    "        infer_ids = [id_ for id_ in infer_tokens_generative(input_ids, model, tokenizer)]\n",
    "        row = [\n",
    "            tokenizer.DecodeIds(ids)\n",
    "            for ids in (input_ids, label_ids, infer_ids)\n",
    "        ]\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测一组 QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    (\n",
    "        '最近晚上睡不着觉，但很困，总觉得不会睡觉？',\n",
    "        '最近一段时间，总是晚上睡不着觉，但是却很困，没次睡觉心理都在不自觉的想着该如何去睡觉，每次想着想着就完全清醒了，失眠特别严重，感觉身体快要透支了，很无助，还连累家人一起跟着担心'\n",
    "    ),\n",
    "    (\n",
    "        '大一，谈了将近一年的女友突然提出分手我该怎么办？',\n",
    "        ' 某一天晚上她突然给我打电话说我不是她喜欢的那种类型，在电话中她多次提到自己的性格不好，说“我所看到的都是加了“滤镜”的效果，其实她并没有我想得那么好，今后不准备结婚”。（她以前谈过一次恋爱，对于那场恋爱她一直都铭记在心）接完电话我很迷茫和困惑，因为之前她愿意跟我分享她的生活中的不如意和困难，突然的转变让我措手不及。同时我也很自责，没有准确的表达出自己的意思和情感，在与她的对话中我常常感到无力和无助。我到底该怎么办？我真的很爱她，想和她度过一生。'\n",
    "    ),\n",
    "    (\n",
    "        '男人出轨',\n",
    "        '我的意思是，出轨者会问自己，我为什么要这么做，被背叛的伴侣会问，你为什么要这样对我?'\n",
    "    ),\n",
    "    (\n",
    "        '我的奇葩婚姻，老公说为了孩子好就是不离婚？',\n",
    "        '我的婚姻最近好像一塌糊涂，可是老公不离婚，我甚至有想过死？我是不是得了抑郁症？和老公从2017年开始就吵吵闹闹，一吵架就分居，分居一两个月老公可以对我和孩子不问不闻，到时间了他就自己又搬回来，但是回来也不和我沟通，就算是上次是他动手，他也不道歉，他特别喜欢冷战。现在和老公又发展到了无性，而且双方家庭也合不来，我和他家人不敢来往，他家人对我要求高，希望天天在家做饭带娃伺候老公，他现在和我父母也不来往，从来不回家吃饭，要等我父母走了他才回来，每天很晚回家，去哪从来不告诉我，我知道他有婚外性，但他從來不承認，还说我是神经病，我還被他傳播上了高危型的HPV，以后是死是活都不确定，我特别想要离婚，我感觉和他生活快要窒息了，他整天拉着个脸，要么对我不闻不问，要么一开口说话就是责怪的语气，但他就是不离婚，说是为了孩子好，还让我不要发神经，我真的不知道该怎么和他相处了。他性格很古怪'\n",
    "    ),\n",
    "    (\n",
    "        '妻子在钱的问题上不坦诚，如何处理好家庭经济问题？',\n",
    "        '和老婆因为意外怀孕结婚，两家谈婚论嫁时闹得不愉快，我家出了一套大户型的房子，一台豪华车（当然这些都是婚前财产），然后给了40万装修婚房，同时给女方10万彩礼。而女方一开始的态度是一分钱没有，后来迫于压力给了50万现金，我老婆名下既没有房也没有车。婚后的钱一直都是给我老婆管理，包括我的工资奖金还有结婚的份子钱等等。我老婆并没有把这些钱看做是家庭财产，就算其中已经掺入了很多我的钱，还是认为全部都是她自己的钱，我只是知道她的卡密码而已，但是卡在哪里，她花了多少钱，花在哪里了，有没有借钱给别人，我通通不知道，这让我很不安。况且她没有理财观念，我曾经三番五次跟她说了理财的重要性，她就只是沉默不语，也不知道是听不懂，还是装傻实际上自己去做了理财但是没把收入情况告诉我，我们多次吵架都是为了钱，我觉得夫妻两个在这个问题上不能坦诚是个大问题，我现在很想经济分开，但是不知道怎么做才比较妥当'\n",
    "    ),\n",
    "    (\n",
    "        '30岁男生，相亲屡次不顺，我该怎么办呢？',\n",
    "        '30岁男生还单身，相亲对象不少，可是次次都以失败收场，自己长相还行，就是不太会说话，和女生聊天刚开始聊的挺不错，没聊几天女的就不回复了，爱答不理，心里郁闷的晚上都想哭，我该怎么办'\n",
    "    ),\n",
    "    (\n",
    "        '30岁女生，离婚念头挥之不去，他为何计划婚内出轨？',\n",
    "        '此刻有点儿失眠心情烦躁，我想了很久没有想明白，为什么他会计划着婚内出轨，……心里，我基本上认定原因应该是我身材不怎么好，胸部不够吸引人，因为婚后发现了在婚前，他约P了一个又肥又长相不佳的女人。我以为就是因为对方胸部傲人。在壹心理一些文章的引导下，我突然明白，源头是性生活不和谐。婚前同居一年多，到婚后2年，性生活基本就是，直奔主题，他一手拿着手机欣赏着他的成人影片一边爱爱，很少会主动关心我的感受，也很少理会我让他收起手机的要求。而且另一方面，他基本每晚都会要求我用嘴亲亲，他自己则拿着手机看片或者无关紧要的东西，让我经常觉得自己就是一个娃娃一个工具。以前我主动跟他说起过几次，然而也只是不了了之。后来我基本对性生活没有兴趣，也很反感为他口J。而且，日常生活变得频繁小吵冷战，很少有之前无话不谈的亲密，我不知道我们还有没有重归于好的机会……'\n",
    "    ),\n",
    "    (\n",
    "        '29岁哺乳期，和公婆因带孩子问题每天崩溃，怎么办？',\n",
    "        '怀孕她就没怎么管，备产各种东西都是自己准备。产后直接是妈妈照顾的。产后半年回来要上班，我爸妈又没退休，无奈下还是他们来照顾。他们方法不对，连个衣服都洗不干净，更不用提孩子的饭了。每天我都自己趁下班时间给孩子准备饭和各种东西。最怕的是习惯，比如她喜欢喂孩子各种炒菜，我说好几次一岁之内吃盐不好就是不听，还故意喂。或者孩子的奶瓶，经常不盖盖子让他把玩，说不卫生，也不搭理。孩子吃饭的碗和勺子永远是扔在各种桌子椅子角落，用的时候直接拿起来用也不洗，洗脸洗脚水每次告诉温度差不多就好了，就是不听，偏要一会冷一会烫。来我家被子也不叠，锅永远是不会洗的，下一顿加水加米接着煮。我老公又每天忙的不在家，我已经要崩溃了'\n",
    "    ),\n",
    "    (\n",
    "        '婚后分居三四年，有过家庭矛盾，我的婚姻是否该继续？',\n",
    "        '婚后分居三四年，和老公之间发生过一些家庭矛盾，可以看我另外一篇疑问。我们上半年闹离婚，后来他又主动和好，但是我们之前除了新婚，几乎很少有夫妻生活，他说一靠近我就觉得心里像一堆蚂蚁抓挠，特别难受。他说这是他得心理问题，小时候受到唾骂和凌辱，造成了心理阴影。我们俩都觉得生活的很痛苦，但为了孩子没有离婚。我跟他在一起就觉得拘束，不自在。向往的温馨家庭生活也得不到，我很累，我应该坚持离婚吗？他这种心理疾病是真的还是借口呢？'\n",
    "    ),\n",
    "#     (\n",
    "#         '',\n",
    "#         '新 闻 大S自嘲上辈子应是儿子的“杀父仇人”，笑说：“不然我怎么一生他就快死了，他是来报仇的吧，而且我一抱他就哭，到三个月后才好一点。”她生下一双儿女集满“好”字，感到非常满足，是否再拼第三胎？大S笑说：“不可能！我现在尽量和老公分房睡。” 评析起初，大家并不看好大S与汪小菲这段婚姻。姐弟恋是一回事儿，更多还是觉得有些不够门当户对。如今，女儿与儿子陆续出生。凑足“好”字的两人，幸福得就像花儿开放。俏江南的变化，对大S与汪小菲毫无影响。男的负责赚钱养家，女的负责照顾孩子开心美满。'\n",
    "#     )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.recompute=True\n",
    "# args.top_p=0.0\n",
    "# args.top_k=0\n",
    "# args.temperature=1.0\n",
    "\n",
    "n_gen = 2\n",
    "\n",
    "for title, text in input_texts:\n",
    "    txt = f'{title}<sep>{text}<|endoftext|>'\n",
    "    context_tokens = tokenizer.EncodeAsIds(txt).tokenization\n",
    "    print(title)\n",
    "    print(text)\n",
    "    print()\n",
    "    for i in range(n_gen):\n",
    "#         args.temperature=random.gauss(0.95, 0.05)\n",
    "        print(f'{i}: ', end='')\n",
    "        s_pred = ''\n",
    "        for s in infer_text_generative(txt, model, tokenizer):\n",
    "            print(s, end='')\n",
    "        print(os.linesep)\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动续写测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "'''\n",
    "婚后分居三四年，有过家庭矛盾，我的婚姻是否该继续？\n",
    "''',\n",
    "'''\n",
    "婚后分居三四年，和老公之间发生过一些家庭矛盾，可以看我另外一篇疑问。\n",
    "我们上半年闹离婚，后来他又主动和好，但是我们之前除了新婚，几乎很少有夫妻生活，他说一靠近我就觉得心里像一堆蚂蚁抓挠，特别难受。他说这是他得心理问题，小时候受到唾骂和凌辱，造成了心理阴影。\n",
    "我们俩都觉得生活的很痛苦，但为了孩子没有离婚。我跟他在一起就觉得拘束，不自在。\n",
    "向往的温馨家庭生活也得不到，我很累，我应该坚持离婚吗？\n",
    "他这种心理疾病是真的还是借口呢？\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这在里写我的回答**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里写一部分\n",
    "answer = '''\n",
    "我觉得这是你\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**执行下面的 cell ，帮我写把上面这句写完，或者写下一句**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pred = 3\n",
    "\n",
    "for i in range(n_pred):\n",
    "    \n",
    "    txt = '<sep>'.join(s.strip() for s in question) + '<|endoftext|>' + '<bos>' + answer.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(txt).tokenization\n",
    "    \n",
    "    print(f'{i+1}: ', end='')\n",
    "    s_pred = ''\n",
    "    with closing(\n",
    "        infer_text_generative(txt, model, tokenizer)\n",
    "    ) as generator:\n",
    "        for s in generator:\n",
    "            if s:\n",
    "                print(s, end='')\n",
    "                if s[-1] in ('.。!！?？'):\n",
    "                    break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
