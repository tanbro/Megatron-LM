{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作评估语料\n",
    "\n",
    "注意输入的 Context tokens 长度要小于最大模型最大生成序列长度的一半！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 代码准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from contextlib import ExitStack\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from glob import glob, iglob\n",
    "from itertools import chain, cycle, islice, count\n",
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQ_LENGTH = 1024\n",
    "MIN_CTX_LEN = 128\n",
    "\n",
    "SPM_MODEL = '../data/spm/gpt2_huamei_corpus_bpe_32k_v2.model'\n",
    "\n",
    "SP = spm.SentencePieceProcessor()\n",
    "SP.load(SPM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_files_line_iterator(paths):\n",
    "    return chain.from_iterable(\n",
    "        open(path)\n",
    "        for path\n",
    "        in tqdm(paths, '[iter files]', unit='file')\n",
    "    )\n",
    "\n",
    "\n",
    "def single_text_file_line_count(path, show_progress_bar=False):\n",
    "    with open(path) as fd:\n",
    "        iterable = tqdm(fd) if show_progress_bar else fd\n",
    "        return sum(1 for _ in iterable)\n",
    "        \n",
    "\n",
    "def text_files_line_count(paths):\n",
    "    try:\n",
    "        total = len(paths)\n",
    "    except (AttributeError, TypeError):\n",
    "        total = None\n",
    "    with Pool() as pool:\n",
    "        it = pool.imap_unordered(\n",
    "            single_text_file_line_count,\n",
    "            tqdm(paths, '[map: files]', unit='file')\n",
    "        )\n",
    "        return sum(c for c in tqdm(it, '[reduce: sum lines]', unit='file', total=total))\n",
    "\n",
    "\n",
    "def proc_line(line):\n",
    "    result = []\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return result\n",
    "    paragraphs = json.loads(line)\n",
    "    text = ''\n",
    "    n_text = 0\n",
    "    for sentence in chain.from_iterable(paragraphs):\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        n_sentence = len(SP.encode_as_ids(sentence))\n",
    "        if n_text + n_sentence < SEQ_LENGTH + MIN_CTX_LEN // 2:\n",
    "            text += sentence\n",
    "            n_text += n_sentence\n",
    "        else:\n",
    "            result.append({'text': text, 'length': n_text})\n",
    "            text = sentence\n",
    "            n_text = n_sentence\n",
    "    if n_text:\n",
    "        result.append({'text': text, 'length': n_text})\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列出输入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f122e0b8454e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m INPUT_FILES = [\n\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     for path in tqdm(iglob(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"/nfs/server01_public/豆瓣/情感相关的小组/data.json/*\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "INPUT_FILES = [\n",
    "    path\n",
    "    for path in tqdm(iglob(\n",
    "        \"/nfs/server01_public/豆瓣/情感相关的小组/data.json/*\",\n",
    "        recursive=True\n",
    "    ))\n",
    "    if os.path.isfile(path) and os.path.splitext(path)[1].lower() in ('json', '.jsonl', '.jsonlines', 'json', 'jsonline')\n",
    "]\n",
    "\n",
    "\n",
    "print(f'源语料文件数：{len(INPUT_FILES):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件采样\n",
    "\n",
    "由于只是用于评估，所以只使用很少的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选取 5 个输入语料文件\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_0(3).jsonl',\n",
       " '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_0(33).jsonl',\n",
       " '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_0(53).jsonl',\n",
       " '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_1(12).jsonl',\n",
       " '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_1(2).jsonl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 5\n",
    "\n",
    "print(f'选取 {K} 个输入语料文件')\n",
    "\n",
    "SRC_FILES = sorted(random.choices(INPUT_FILES, k=5))\n",
    "\n",
    "SRC_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件修复\n",
    "\n",
    "由于格式错误，需要修复！修复后的文件保存到来源文件相同的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc4658327694e73b97f699ba21eef51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='map', max=5, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ddb53d8ff549d6830eb25fc18e78bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='wait', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "修复后的文件：['/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_0(3).jsonl.fix', '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_0(33).jsonl.fix', '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_0(53).jsonl.fix', '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_1(12).jsonl.fix', '/nfs/server01_public/豆瓣/情感相关的小组/data.json/情感相关的小组-话题_url_1(2).jsonl.fix']\n"
     ]
    }
   ],
   "source": [
    "FIXED_FILES = [fn + '.fix' for fn in SRC_FILES]\n",
    "\n",
    "\n",
    "def fix_json(args):\n",
    "    src_fn, dst_fn = args\n",
    "    with open(src_fn) as src_fp, open(dst_fn, 'w') as dst_fp:\n",
    "        for line in src_fp:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                data=eval(line)\n",
    "            text = json.dumps(data, ensure_ascii=False)\n",
    "            print(text, file=dst_fp)\n",
    "\n",
    "with Pool() as pool:\n",
    "    it = pool.imap_unordered(\n",
    "        fix_json,\n",
    "        tqdm(zip(SRC_FILES, FIXED_FILES), 'map', total=len(SRC_FILES)),\n",
    "    )\n",
    "    for _ in tqdm(it, 'wait', total=len(SRC_FILES)):\n",
    "        pass\n",
    "\n",
    "\n",
    "print(f'修复后的文件：{FIXED_FILES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计输入文件总行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bc7c29611948a7b48692e88cae342e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[map: files]', max=5, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5446576c36494980a224b09b7b428153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[reduce: sum lines]', max=5, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "源语料行数：7,788\n",
      "CPU times: user 73.5 ms, sys: 38 ms, total: 112 ms\n",
      "Wall time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "total_lines = text_files_line_count(FIXED_FILES)\n",
    "print(f'源语料行数：{total_lines:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理\n",
    "\n",
    "我们目前的评估目标\n",
    "\n",
    "1. 进行人工评估，输出列表文件进行比对\n",
    "1. 输入文字为回答数据的逗号前的半句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def proc_line(line):\n",
    "    result = []\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return result\n",
    "    #\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def ptb_detokenizer(string):\n",
    "\tstring = string.replace(\" '\", \"'\")\n",
    "\tstring = string.replace(\" \\n\", \"\\n\")\n",
    "\tstring = string.replace(\"\\n \", \"\\n\")\n",
    "\tstring = string.replace(\" n't\", \"n't\")\n",
    "\tstring = string.replace(\" N \",\"1 \")\n",
    "\tstring = string.replace(\"$ 1\", \"$1\")\n",
    "\tstring = string.replace(\"# 1\", \"#1\")\n",
    "\treturn string\n",
    "\n",
    "\n",
    "def wikitext_detokenizer(string):\n",
    "\t#contractions\n",
    "\tstring = string.replace(\"s '\", \"s'\")\n",
    "\tstring = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n",
    "\t# number separators\n",
    "\tstring = string.replace(\" @-@ \", \"-\")\n",
    "\tstring = string.replace(\" @,@ \", \",\")\n",
    "\tstring = string.replace(\" @.@ \", \".\")\n",
    "\t#punctuation\n",
    "\tstring = string.replace(\" : \", \": \")\n",
    "\tstring = string.replace(\" ; \", \"; \")\n",
    "\tstring = string.replace(\" . \", \". \")\n",
    "\tstring = string.replace(\" ! \", \"! \")\n",
    "\tstring = string.replace(\" ? \", \"? \")\n",
    "\tstring = string.replace(\" , \", \", \")\n",
    "\t# double brackets\n",
    "\tstring = re.sub(r\"\\(\\s*([^\\)]*?)\\s*\\)\", r\"(\\1)\", string)\n",
    "\tstring = re.sub(r\"\\[\\s*([^\\]]*?)\\s*\\]\", r\"[\\1]\", string)\n",
    "\tstring = re.sub(r\"{\\s*([^}]*?)\\s*}\", r\"{\\1}\", string)\n",
    "\tstring = re.sub(r\"\\\"\\s*([^\\\"]*?)\\s*\\\"\", r'\"\\1\"', string)\n",
    "\tstring = re.sub(r\"'\\s*([^']*?)\\s*'\", r\"'\\1'\", string)\n",
    "\t# miscellaneous\n",
    "\tstring = string.replace(\"= = = =\", \"====\")\n",
    "\tstring = string.replace(\"= = =\", \"===\")\n",
    "\tstring = string.replace(\"= =\", \"==\")\n",
    "\tstring = string.replace(\" \"+chr(176)+\" \", chr(176))\n",
    "\tstring = string.replace(\" \\n\", \"\\n\")\n",
    "\tstring = string.replace(\"\\n \", \"\\n\")\n",
    "\tstring = string.replace(\" N \", \" 1 \")\n",
    "\tstring = string.replace(\" 's\", \"'s\")\n",
    "\n",
    "\treturn string\n",
    "\n",
    "\n",
    "CJK_WHITESPACE_REGEX = re.compile(r'(?P<c>[\\u2E80-\\u9FFF])(\\s+)')\n",
    "def remove_cjk_whitespace(s):  # type: (str)->str\n",
    "    return re.sub(CJK_WHITESPACE_REGEX, r'\\g<c>', s.strip())\n",
    "\n",
    "\n",
    "REPLACEMENT_CHARACTER_REGEX = re.compile(r'\\uFFFD')\n",
    "def replace_character(s, repl):  # type: (str)->str\n",
    "    return re.sub(REPLACEMENT_CHARACTER_REGEX, repl, s)\n",
    "\n",
    "\n",
    "REPLACEMENT_CHARACTER = chr(0XFFFD)\n",
    "\n",
    "def normalize(s):\n",
    "    s = s.strip()\n",
    "    # remove html tags\n",
    "    s = BeautifulSoup(s).get_text(os.linesep)\n",
    "    # 消除空行，空的空格\n",
    "    _s = ''\n",
    "    for sn in s.split():\n",
    "        sn = sn.strip()\n",
    "        if not sn: continue\n",
    "        if sn \n",
    "    s = _s\n",
    "    # 消除中文之间的空格\n",
    "    s = remove_cjk_whitespace(s)\n",
    "    #\n",
    "#     s = ptb_detokenizer(s)\n",
    "#     s = wikitext_detokenizer(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'2010-05-25 09:28:17 安米(心有阳光\\n他们喜欢\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"'2010-05-25 09:28:17 安米(心有阳光�他们喜欢\"\n",
    "REPLACEMENT_CHARACTER_REGEX = re.compile(r'\\uFFFD')\n",
    "re.sub(REPLACEMENT_CHARACTER_REGEX, r'\\n', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2010-05-25 09:28:17 安米 (心有阳光，自暖人) 他们喜欢突然冷下来，让你措手不及\\n======\\n是的！\\n前一秒可以跟你疯的要死。下一秒直接转头不认人'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = normalize(' 2010-05-25 09:28:17 安米 (心有阳光，自暖人) 他们喜欢突然冷下来，让你措手不及<br>======<br>是的！<br>前一秒可以跟你疯的要死。下一秒直接转头不认人<br>')\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['他们喜欢突然冷下来，让你措手不及',\n",
       " ' 2010-05-25 09:28:17 安米 (心有阳光，自暖人) 他们喜欢突然冷下来，让你措手不及<br>======<br>是的！<br>前一秒可以跟你疯的要死。下一秒直接转头不认人<br>',\n",
       " '今天双鱼跟我分手，因为年龄的原因，他的父母无法接受，他短期说服不了。。。<br>他能等，我不能等。。。。于是害怕耽误我。。。。<br>怎么办<br>',\n",
       " '你给鱼一分的好，他会还你十分的爱，但不鱼很敏感，甚至自欺，不要让他感觉你冷落了他，你越粘他，越在乎他，让他感觉到',\n",
       " '我家的鱼腩是属于不喜欢你过多干涉他的 过多的关心他会烦<br>反而比较喜欢把心思花在我身上 逗我玩<br>提到他的事 就比较温吞和犹疑了 我关心几句他会很开心 多了他就会说知道了知道了 然后转移话题<br>我生气的时候也说过 相处不来就别相处了 是很冷静的说的 <br>然后他就会贱贱的说 不要嘛老婆 <br>有时候被我逼急了 他也会不开心 摔电话<br>但很快就会打来电话 百般求饶 <br>唉。。。。。。。他的好脾气是我的最爱']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d['anwser'] for d in data['anwsers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic_id': '10935952',\n",
       " 'url': 'www.douban.com/group/topic/10935952/',\n",
       " 'date': '2010-04-22 00:48:17',\n",
       " 'title': '直播鱼之相处心理分析',\n",
       " 'text': '\"这里设定三种。一鱼和你互为真爱，你们只计较过去，担心未来。二你爱鱼很多，它爱你一般。三鱼爱你很多，你烦。（第三种就不用分析了吧）<br>我是第一种。起码现阶段是。以后只可能有第二种趋势。烦劳众夜光鱼不吝赐教。共商鱼际。什么偏题的水楼咱们尽量少好哇。<br><br>我来提第一个。<br>近日进入和鱼的深入了解阶段。什么叫深入了解，就是不停留在纯粹的思想交流上，互相表达爱慕之情（我更多些）上。已经开始打情骂俏，晚上希望能腻歪几句的地步。可是！我鱼说：“守着我”。这种彻头彻尾的索取鬼越亲我越担心他会一得不到足够的爱感就劈腿。我家鱼不比在座的各位逊色在暧昧这方面<br>所以：我的担心是对的不？我该怎么做？我很爱很爱鱼。想和他一辈子。',\n",
       " 'author': '夏澈澈',\n",
       " 'anwsers': [{'anwserer': '\"杰米\"',\n",
       "   'likes': 0,\n",
       "   'anwsertime': '\"2010-05-25 09:28:17',\n",
       "   'anwser': '他们喜欢突然冷下来，让你措手不及',\n",
       "   'directtext': ''},\n",
       "  {'anwserer': '\"多啦ZZZZ梦\"',\n",
       "   'likes': 0,\n",
       "   'anwsertime': '\"2010-05-26 19:38:48',\n",
       "   'anwser': ' 2010-05-25 09:28:17 安米 (心有阳光，自暖人) 他们喜欢突然冷下来，让你措手不及<br>======<br>是的！<br>前一秒可以跟你疯的要死。下一秒直接转头不认人<br>',\n",
       "   'directtext': ''},\n",
       "  {'anwserer': '\"monicpan\"',\n",
       "   'likes': 0,\n",
       "   'anwsertime': '\"2010-05-26 20:36:56',\n",
       "   'anwser': '今天双鱼跟我分手，因为年龄的原因，他的父母无法接受，他短期说服不了。。。<br>他能等，我不能等。。。。于是害怕耽误我。。。。<br>怎么办<br>',\n",
       "   'directtext': ''},\n",
       "  {'anwserer': '\"有个外号叫小白\"',\n",
       "   'likes': 0,\n",
       "   'anwsertime': '\"2010-05-26 20:43:22',\n",
       "   'anwser': '你给鱼一分的好，他会还你十分的爱，但不鱼很敏感，甚至自欺，不要让他感觉你冷落了他，你越粘他，越在乎他，让他感觉到',\n",
       "   'directtext': ''},\n",
       "  {'anwserer': '\"未央她不在\"',\n",
       "   'likes': 0,\n",
       "   'anwsertime': '\"2010-05-26 20:51:05',\n",
       "   'anwser': '我家的鱼腩是属于不喜欢你过多干涉他的 过多的关心他会烦<br>反而比较喜欢把心思花在我身上 逗我玩<br>提到他的事 就比较温吞和犹疑了 我关心几句他会很开心 多了他就会说知道了知道了 然后转移话题<br>我生气的时候也说过 相处不来就别相处了 是很冷静的说的 <br>然后他就会贱贱的说 不要嘛老婆 <br>有时候被我逼急了 他也会不开心 摔电话<br>但很快就会打来电话 百般求饶 <br>唉。。。。。。。他的好脾气是我的最爱',\n",
       "   'directtext': ''}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FIXED_FILES[0]) as fp:\n",
    "    for line in fp:\n",
    "        data = json.loads(line)\n",
    "        break\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = '../data/gpt2_huamei_corpus_emotion.jsonl'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
