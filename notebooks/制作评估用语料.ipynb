{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作评估语料\n",
    "\n",
    "注意输入的 Context tokens 长度要小于最大模型最大生成序列长度的一半！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 代码准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from contextlib import ExitStack\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from glob import glob, iglob\n",
    "from itertools import chain, cycle, islice, count\n",
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 1024\n",
    "MIN_CTX_LEN = 128\n",
    "\n",
    "SPM_MODEL = '../data/spm/gpt2_huamei_corpus_bpe_32k_v2.model'\n",
    "\n",
    "SP = spm.SentencePieceProcessor()\n",
    "SP.load(SPM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_files_line_iterator(paths):\n",
    "    return chain.from_iterable(\n",
    "        open(path)\n",
    "        for path\n",
    "        in tqdm(paths, '[iter files]', unit='file')\n",
    "    )\n",
    "\n",
    "\n",
    "def single_text_file_line_count(path, show_progress_bar=False):\n",
    "    with open(path) as fd:\n",
    "        iterable = tqdm(fd) if show_progress_bar else fd\n",
    "        return sum(1 for _ in iterable)\n",
    "        \n",
    "\n",
    "def text_files_line_count(paths):\n",
    "    try:\n",
    "        total = len(paths)\n",
    "    except (AttributeError, TypeError):\n",
    "        total = None\n",
    "    with Pool() as pool:\n",
    "        it = pool.imap_unordered(\n",
    "            single_text_file_line_count,\n",
    "            tqdm(paths, '[map: files]', unit='file')\n",
    "        )\n",
    "        return sum(c for c in tqdm(it, '[reduce: sum lines]', unit='file', total=total))\n",
    "\n",
    "\n",
    "def proc_line(line):\n",
    "    result = []\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return result\n",
    "    paragraphs = json.loads(line)\n",
    "    text = ''\n",
    "    n_text = 0\n",
    "    for sentence in chain.from_iterable(paragraphs):\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        n_sentence = len(SP.encode_as_ids(sentence))\n",
    "        if n_text + n_sentence < SEQ_LENGTH + MIN_CTX_LEN // 2:\n",
    "            text += sentence\n",
    "            n_text += n_sentence\n",
    "        else:\n",
    "            result.append({'text': text, 'length': n_text})\n",
    "            text = sentence\n",
    "            n_text = n_sentence\n",
    "    if n_text:\n",
    "        result.append({'text': text, 'length': n_text})\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列出输入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILES = [\n",
    "    path\n",
    "    for path in tqdm(iglob(\n",
    "        \"/nfs/server01_public/豆瓣/情感相关的小组/data.json/*\",\n",
    "        recursive=True\n",
    "    ))\n",
    "    if os.path.isfile(path) and os.path.splitext(path)[1].lower() in ('json', '.jsonl', '.jsonlines', 'json', 'jsonline')\n",
    "]\n",
    "\n",
    "\n",
    "print(f'源语料文件数：{len(INPUT_FILES):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件采样\n",
    "\n",
    "由于只是用于评估，所以只使用很少的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "\n",
    "print(f'选取 {K} 个输入语料文件')\n",
    "\n",
    "SRC_FILES = sorted(random.choices(INPUT_FILES, k=5))\n",
    "\n",
    "SRC_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件修复\n",
    "\n",
    "由于格式错误，需要修复！修复后的文件保存到来源文件相同的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_FILES = [fn + '.fix' for fn in SRC_FILES]\n",
    "\n",
    "\n",
    "def fix_json(args):\n",
    "    src_fn, dst_fn = args\n",
    "    with open(src_fn) as src_fp, open(dst_fn, 'w') as dst_fp:\n",
    "        for line in src_fp:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                data=eval(line)\n",
    "            text = json.dumps(data, ensure_ascii=False)\n",
    "            print(text, file=dst_fp)\n",
    "\n",
    "with Pool() as pool:\n",
    "    it = pool.imap_unordered(\n",
    "        fix_json,\n",
    "        tqdm(zip(SRC_FILES, FIXED_FILES), 'map', total=len(SRC_FILES)),\n",
    "    )\n",
    "    for _ in tqdm(it, 'wait', total=len(SRC_FILES)):\n",
    "        pass\n",
    "\n",
    "\n",
    "print(f'修复后的文件：{FIXED_FILES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计输入文件总行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "total_lines = text_files_line_count(FIXED_FILES)\n",
    "print(f'源语料行数：{total_lines:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理\n",
    "\n",
    "我们目前的评估目标\n",
    "\n",
    "1. 进行人工评估，输出列表文件进行比对\n",
    "1. 输入文字为回答数据的逗号前的半句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def proc_line(line):\n",
    "    result = []\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return result\n",
    "    #\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def ptb_detokenizer(string):\n",
    "\tstring = string.replace(\" '\", \"'\")\n",
    "\tstring = string.replace(\" \\n\", \"\\n\")\n",
    "\tstring = string.replace(\"\\n \", \"\\n\")\n",
    "\tstring = string.replace(\" n't\", \"n't\")\n",
    "\tstring = string.replace(\" N \",\"1 \")\n",
    "\tstring = string.replace(\"$ 1\", \"$1\")\n",
    "\tstring = string.replace(\"# 1\", \"#1\")\n",
    "\treturn string\n",
    "\n",
    "\n",
    "def wikitext_detokenizer(string):\n",
    "\t#contractions\n",
    "\tstring = string.replace(\"s '\", \"s'\")\n",
    "\tstring = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n",
    "\t# number separators\n",
    "\tstring = string.replace(\" @-@ \", \"-\")\n",
    "\tstring = string.replace(\" @,@ \", \",\")\n",
    "\tstring = string.replace(\" @.@ \", \".\")\n",
    "\t#punctuation\n",
    "\tstring = string.replace(\" : \", \": \")\n",
    "\tstring = string.replace(\" ; \", \"; \")\n",
    "\tstring = string.replace(\" . \", \". \")\n",
    "\tstring = string.replace(\" ! \", \"! \")\n",
    "\tstring = string.replace(\" ? \", \"? \")\n",
    "\tstring = string.replace(\" , \", \", \")\n",
    "\t# double brackets\n",
    "\tstring = re.sub(r\"\\(\\s*([^\\)]*?)\\s*\\)\", r\"(\\1)\", string)\n",
    "\tstring = re.sub(r\"\\[\\s*([^\\]]*?)\\s*\\]\", r\"[\\1]\", string)\n",
    "\tstring = re.sub(r\"{\\s*([^}]*?)\\s*}\", r\"{\\1}\", string)\n",
    "\tstring = re.sub(r\"\\\"\\s*([^\\\"]*?)\\s*\\\"\", r'\"\\1\"', string)\n",
    "\tstring = re.sub(r\"'\\s*([^']*?)\\s*'\", r\"'\\1'\", string)\n",
    "\t# miscellaneous\n",
    "\tstring = string.replace(\"= = = =\", \"====\")\n",
    "\tstring = string.replace(\"= = =\", \"===\")\n",
    "\tstring = string.replace(\"= =\", \"==\")\n",
    "\tstring = string.replace(\" \"+chr(176)+\" \", chr(176))\n",
    "\tstring = string.replace(\" \\n\", \"\\n\")\n",
    "\tstring = string.replace(\"\\n \", \"\\n\")\n",
    "\tstring = string.replace(\" N \", \" 1 \")\n",
    "\tstring = string.replace(\" 's\", \"'s\")\n",
    "\n",
    "\treturn string\n",
    "\n",
    "\n",
    "CJK_WHITESPACE_REGEX = re.compile(r'(?P<c>[\\u2E80-\\u9FFF])(\\s+)')\n",
    "def remove_cjk_whitespace(s):  # type: (str)->str\n",
    "    return re.sub(CJK_WHITESPACE_REGEX, r'\\g<c>', s.strip())\n",
    "\n",
    "\n",
    "REPLACEMENT_CHARACTER_REGEX = re.compile(r'\\uFFFD')\n",
    "def replace_character(s, repl):  # type: (str)->str\n",
    "    return re.sub(REPLACEMENT_CHARACTER_REGEX, repl, s)\n",
    "\n",
    "\n",
    "REPLACEMENT_CHARACTER = chr(0XFFFD)\n",
    "\n",
    "def normalize(s):\n",
    "    s = s.strip()\n",
    "    # remove html tags\n",
    "    s = BeautifulSoup(s).get_text(os.linesep)\n",
    "    # 消除空行，空的空格\n",
    "    _s = ''\n",
    "    for sn in s.split():\n",
    "        sn = sn.strip()\n",
    "        if not sn: continue\n",
    "        if sn \n",
    "    s = _s\n",
    "    # 消除中文之间的空格\n",
    "    s = remove_cjk_whitespace(s)\n",
    "    #\n",
    "#     s = ptb_detokenizer(s)\n",
    "#     s = wikitext_detokenizer(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"'2010-05-25 09:28:17 安米(心有阳光�他们喜欢\"\n",
    "REPLACEMENT_CHARACTER_REGEX = re.compile(r'\\uFFFD')\n",
    "re.sub(REPLACEMENT_CHARACTER_REGEX, r'\\n', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = normalize(' 2010-05-25 09:28:17 安米 (心有阳光，自暖人) 他们喜欢突然冷下来，让你措手不及<br>======<br>是的！<br>前一秒可以跟你疯的要死。下一秒直接转头不认人<br>')\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d['anwser'] for d in data['anwsers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FIXED_FILES[0]) as fp:\n",
    "    for line in fp:\n",
    "        data = json.loads(line)\n",
    "        break\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = '../data/gpt2_huamei_corpus_emotion.jsonl'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
