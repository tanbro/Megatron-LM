{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作评估语料\n",
    "\n",
    "注意输入的 Context tokens 长度要小于最大模型最大生成序列长度的一半！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 代码准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from contextlib import ExitStack\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from glob import glob, iglob\n",
    "from itertools import chain, cycle, islice, count\n",
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQ_LENGTH = 1024\n",
    "MIN_CTX_LEN = 128\n",
    "\n",
    "SPM_MODEL = '../data/spm/gpt2_huamei_corpus_bpe_32k_v2.model'\n",
    "\n",
    "\n",
    "SP = spm.SentencePieceProcessor()\n",
    "SP.load(SPM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_files_line_iterator(paths):\n",
    "    return chain.from_iterable(\n",
    "        open(path)\n",
    "        for path\n",
    "        in tqdm(paths, '[iter files]', unit='file')\n",
    "    )\n",
    "\n",
    "\n",
    "def single_text_file_line_count(path, show_progress_bar=False):\n",
    "    with open(path) as fd:\n",
    "        iterable = tqdm(fd) if show_progress_bar else fd\n",
    "        return sum(1 for _ in iterable)\n",
    "        \n",
    "\n",
    "def text_files_line_count(paths):\n",
    "    try:\n",
    "        total = len(paths)\n",
    "    except (AttributeError, TypeError):\n",
    "        total = None\n",
    "    with Pool() as pool:\n",
    "        it = pool.imap_unordered(\n",
    "            single_text_file_line_count,\n",
    "            tqdm(paths, '[map files ]', unit='file')\n",
    "        )\n",
    "        return sum(c for c in tqdm(it, '[sum files ]', unit='file', total=total))\n",
    "\n",
    "\n",
    "def proc_line(line):\n",
    "    result = []\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return result\n",
    "    paragraphs = json.loads(line)\n",
    "    text = ''\n",
    "    n_text = 0\n",
    "    for sentence in chain.from_iterable(paragraphs):\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        n_sentence = len(SP.encode_as_ids(sentence))\n",
    "        if n_text + n_sentence < SEQ_LENGTH + MIN_CTX_LEN // 2:\n",
    "            text += sentence\n",
    "            n_text += n_sentence\n",
    "        else:\n",
    "            result.append({'text': text, 'length': n_text})\n",
    "            text = sentence\n",
    "            n_text = n_sentence\n",
    "    if n_text:\n",
    "        result.append({'text': text, 'length': n_text})\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列出输入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf499f8cc8b641739ce5a3089cc27769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "源语料文件数：379\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILES = [\n",
    "    path\n",
    "    for path in tqdm(iglob(\n",
    "        \"/nfs/server01_public/data/gpt2/output/gpt2_huamei_corpus.json.8g/**/*.*\",\n",
    "        recursive=True\n",
    "    ))\n",
    "    if os.path.isfile(path) and os.path.splitext(path)[1].lower() in ('.jsonl', '.jsonlines', 'json', 'jsonline')\n",
    "]\n",
    "print(f'源语料文件数：{len(INPUT_FILES):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计输入文件总行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2e9eb9ac8d47139365c9d21966c7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[map files ]', max=379, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632793903ae3487388f923ea9defc6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[sum files ]', max=379, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "源语料行数：1,710,236\n",
      "CPU times: user 142 ms, sys: 48 ms, total: 190 ms\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "total_lines = text_files_line_count(INPUT_FILES)\n",
    "print(f'源语料行数：{total_lines:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = '../data/gpt2_huamei_corpus_emotion.jsonl'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
