{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron 的 Dataset 机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测时，可以使用的 `CUDA` 设备:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from utils import print_args\n",
    "from pretrain_gpt2 import initialize_distributed, set_random_seed, get_train_val_test_data, setup_model_and_optimizer, get_masks_and_position_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    num_layers=12,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=1024,\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=512,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "    tokenizer_type=SentencePieceTokenizer,\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    train_data=\"data/xinli001_qa-dev.json\",\n",
    "    lazy_loader=None,\n",
    "    valid_data=None,\n",
    "    test_data=None,\n",
    "    use_tfrecords=None,\n",
    "    presplit_sentences=None,\n",
    "    num_workers=2,\n",
    "    split = '949,50,1',\n",
    "    delim=',',\n",
    "    cache_dir=None,\n",
    "    text_key='text',\n",
    "    eval_text_key=None,\n",
    "    loose_json=True,\n",
    "    max_preds_per_seq=None,\n",
    "\n",
    "    # Training arguments.\n",
    "    load='./checkpoints/gpt2-117m-emotion',\n",
    "    save='./checkpoints/gpt2-117m-xinli001_qa.finetune',\n",
    "    save_interval=5000,    \n",
    "    batch_size=4,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=True,\n",
    "    release=None,\n",
    "    resume_dataloader=None,\n",
    "    no_save_optim=None,\n",
    "    no_save_rng=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    shuffle=None,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None,\n",
    "    adlr_autoresume=None,\n",
    "    weight_decay=0.01,\n",
    "    clip_grad=1.0,\n",
    "    train_iters=1000000,\n",
    "    log_interval=100,\n",
    "    exit_interval=None,\n",
    "    seed=1234,\n",
    "    lr_decay_iters=None,\n",
    "    lr_decay_style='cosine',\n",
    "    lr=0.00015,\n",
    "    min_lr=0.0,\n",
    "    warmup=0.01,\n",
    "    override_lr_scheduler=None,\n",
    "    use_checkpoint_lr_scheduler=None,\n",
    "    use_npy_data_loader = False,\n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "    out_seq_length=128,\n",
    "\n",
    "    # Evaluation arguments.\n",
    "    eval_batch_size=None,\n",
    "    eval_iters=100,\n",
    "    eval_interval=1000,\n",
    "    eval_seq_length=None,\n",
    "    eval_max_preds_per_seq=None,\n",
    "    overlapping_eval=32,\n",
    "    cloze_eval=None,\n",
    "    strict_lambada=None,\n",
    "    eval_hf=None,\n",
    "    load_openai=None,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 data/xinli001_qa-dev.json\n"
     ]
    }
   ],
   "source": [
    "! wc -l {args.train_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.8 ms, sys: 11.5 ms, total: 85.3 ms\n",
      "Wall time: 83.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = make_tokenizer(SentencePieceTokenizer, None, model_path=args.tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "Pretrain GPT2 model\n",
      "arguments:\n",
      "  num_layers ................... 12\n",
      "  hidden_size .................. 768\n",
      "  num_attention_heads .......... 12\n",
      "  max_position_embeddings ...... 1024\n",
      "  vocab_size ................... None\n",
      "  make_vocab_size_divisible_by . 128\n",
      "  attention_dropout ............ 0.1\n",
      "  hidden_dropout ............... 0.1\n",
      "  seq_length ................... 512\n",
      "  model_parallel_size .......... 1\n",
      "  tokenizer_model_type ......... bert-large-uncased\n",
      "  tokenizer_type ............... <class 'data_utils.tokenization.SentencePieceTokenizer'>\n",
      "  tokenizer_path ............... ./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\n",
      "  train_data ................... data/xinli001_qa-dev.json\n",
      "  lazy_loader .................. None\n",
      "  valid_data ................... None\n",
      "  test_data .................... None\n",
      "  use_tfrecords ................ None\n",
      "  presplit_sentences ........... None\n",
      "  num_workers .................. 2\n",
      "  split ........................ 949,50,1\n",
      "  delim ........................ ,\n",
      "  cache_dir .................... None\n",
      "  text_key ..................... text\n",
      "  eval_text_key ................ None\n",
      "  loose_json ................... True\n",
      "  max_preds_per_seq ............ None\n",
      "  load ......................... ./checkpoints/gpt2-117m-emotion\n",
      "  save ......................... ./checkpoints/gpt2-117m-xinli001_qa.finetune\n",
      "  save_interval ................ 5000\n",
      "  batch_size ................... 4\n",
      "  checkpoint_activations ....... None\n",
      "  checkpoint_num_layers ........ 1\n",
      "  finetune ..................... True\n",
      "  release ...................... None\n",
      "  resume_dataloader ............ None\n",
      "  no_save_optim ................ None\n",
      "  no_save_rng .................. None\n",
      "  no_load_optim ................ None\n",
      "  no_load_rng .................. None\n",
      "  fp16 ......................... True\n",
      "  hysteresis ................... 2\n",
      "  loss_scale ................... None\n",
      "  loss_scale_window ............ 1000\n",
      "  min_scale .................... 1\n",
      "  shuffle ...................... None\n",
      "  distributed_backend .......... nccl\n",
      "  DDP_impl ..................... local\n",
      "  local_rank ................... None\n",
      "  reset_position_ids ........... None\n",
      "  reset_attention_mask ......... None\n",
      "  eod_mask_loss ................ None\n",
      "  adlr_autoresume .............. None\n",
      "  weight_decay ................. 0.01\n",
      "  clip_grad .................... 1.0\n",
      "  train_iters .................. 1000000\n",
      "  log_interval ................. 100\n",
      "  exit_interval ................ None\n",
      "  seed ......................... 1234\n",
      "  lr_decay_iters ............... None\n",
      "  lr_decay_style ............... cosine\n",
      "  lr ........................... 0.00015\n",
      "  min_lr ....................... 0.0\n",
      "  warmup ....................... 0.01\n",
      "  override_lr_scheduler ........ None\n",
      "  use_checkpoint_lr_scheduler .. None\n",
      "  use_npy_data_loader .......... False\n",
      "  recompute .................... None\n",
      "  greedy ....................... False\n",
      "  top_p ........................ 0.0\n",
      "  top_k ........................ 0\n",
      "  temperature .................. 1.0\n",
      "  out_seq_length ............... 128\n",
      "  eval_batch_size .............. None\n",
      "  eval_iters ................... 100\n",
      "  eval_interval ................ 1000\n",
      "  eval_seq_length .............. None\n",
      "  eval_max_preds_per_seq ....... None\n",
      "  overlapping_eval ............. 32\n",
      "  cloze_eval ................... None\n",
      "  strict_lambada ............... None\n",
      "  eval_hf ...................... None\n",
      "  load_openai .................. None\n",
      "  cuda ......................... True\n",
      "  rank ......................... 0\n",
      "  world_size ................... 1\n",
      "  dynamic_loss_scale ........... True\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "CPU times: user 2.04 s, sys: 606 ms, total: 2.65 s\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Pytorch distributed.\n",
    "initialize_distributed(args)\n",
    "if torch.distributed.get_rank() == 0:\n",
    "    print('Pretrain GPT2 model')\n",
    "    print_args(args)\n",
    "\n",
    "# Autoresume.\n",
    "torch.distributed.barrier()\n",
    "if args.adlr_autoresume:\n",
    "    enable_adlr_autoresume(args)\n",
    "\n",
    "# Random seeds for reproducibility.\n",
    "set_random_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, and learning rate.\n",
    "# model, optimizer, lr_scheduler = setup_model_and_optimizer(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuring data\n",
      "> padded vocab (size: 32008) with 120 dummy tokens (new size: 32128)\n",
      "> found end-of-document token: 1\n"
     ]
    }
   ],
   "source": [
    "# Data stuff.\n",
    "(\n",
    "    train_data, val_data, test_data, args.vocab_size, args.eod_token\n",
    ") = get_train_val_test_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "250\n",
      "250\n",
      "32128\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for ds in (train_data, val_data, test_data):\n",
    "    print(len(ds))\n",
    "\n",
    "print(args.vocab_size)\n",
    "print(args.eod_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': tensor([[ 8878,  7992,  4793,  ...,  8852,  8880,  8785],\n",
       "         [ 8785,   187, 10440,  ...,  8785,   494,   535],\n",
       "         [ 8962,  9408,   853,  ...,  3044,  9817,  9779],\n",
       "         [ 8993,  9787,  8856,  ...,  9831,  8878,  8785]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iterator = iter(train_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 880, 5547, 2081,  ..., 2431, 1571, 8878],\n",
       "        [9025, 8913, 8872,  ..., 8836, 1595, 2652],\n",
       "        [8785, 2428, 1701,  ...,  297,   57,  686],\n",
       "        [ 115, 2379, 8830,  ...,   58, 9635, 8818]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Items and their type.\n",
    "keys = ['text']\n",
    "datatype = torch.int64\n",
    "\n",
    "\n",
    "data = next(data_iterator)\n",
    "data_b = mpu.broadcast_data(keys, data, datatype)\n",
    "\n",
    "# Unpack.\n",
    "tokens_ = data_b['text'].long()\n",
    "labels = tokens_[:, 1:].contiguous()\n",
    "tokens = tokens_[:, :-1].contiguous()\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the masks and postition ids.\n",
    "attention_mask, loss_mask, position_ids = get_masks_and_position_ids(\n",
    "    tokens,\n",
    "    args.eod_token,\n",
    "    args.reset_position_ids,\n",
    "    args.reset_attention_mask,\n",
    "    args.eod_mask_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]], device='cuda:0'),\n",
       " tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0'),\n",
       " tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
       "         [  0,   1,   2,  ..., 509, 510, 511],\n",
       "         [  0,   1,   2,  ..., 509, 510, 511],\n",
       "         [  0,   1,   2,  ..., 509, 510, 511]], device='cuda:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask, loss_mask, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
