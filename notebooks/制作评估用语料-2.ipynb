{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作评估语料\n",
    "\n",
    "注意输入的 Context tokens 长度要小于最大模型最大生成序列长度的一半！\n",
    "\n",
    "这个 notebook 针对的输入 corpus 文件是：\n",
    "\n",
    "- 每个文档一行\n",
    "- 文档包含段落数组\n",
    "- 段落包含句子数组\n",
    "- 从逗号中间截断，得出输入文本\n",
    "\n",
    "的 Loose JSON 文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 代码准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "from contextlib import ExitStack\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from glob import glob, iglob\n",
    "from itertools import chain, cycle, islice, count, accumulate, compress, repeat\n",
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from ftfy import fix_text\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "MIN_LEN = 16\n",
    "\n",
    "SPM_MODEL = '../data/spm/gpt2_huamei_corpus_bpe_32k_v2.model'\n",
    "\n",
    "SP = spm.SentencePieceProcessor()\n",
    "SP.load(SPM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_files_line_iterator(paths):\n",
    "    return chain.from_iterable(\n",
    "        open(path)\n",
    "        for path\n",
    "        in tqdm(paths, '[iter files]', unit='file')\n",
    "    )\n",
    "\n",
    "\n",
    "def single_text_file_line_count(path, show_progress_bar=False):\n",
    "    with open(path) as fd:\n",
    "        iterable = tqdm(fd) if show_progress_bar else fd\n",
    "        return sum(1 for _ in iterable)\n",
    "        \n",
    "\n",
    "def text_files_line_count(paths):\n",
    "    try:\n",
    "        total = len(paths)\n",
    "    except (AttributeError, TypeError):\n",
    "        total = None\n",
    "    with Pool() as pool:\n",
    "        it = pool.imap_unordered(\n",
    "            single_text_file_line_count,\n",
    "            tqdm(paths, '[map: files]', unit='file')\n",
    "        )\n",
    "        return sum(c for c in tqdm(it, '[reduce: sum lines]', unit='file', total=total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列出输入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_FILES = [\n",
    "#     path\n",
    "#     for path in tqdm(iglob(\n",
    "#         \"/nfs/server01_public/豆瓣/情感相关的小组/data.json/*\",\n",
    "#         recursive=True\n",
    "#     ))\n",
    "#     if os.path.isfile(path) and os.path.splitext(path)[1].lower() in ('json', '.jsonl', '.jsonlines', 'json', 'jsonline')\n",
    "# ]\n",
    "\n",
    "INPUT_FILES = [\n",
    "    '/nfs/server01_public/data/gpt2/output/xinli_20191016.jsonl'\n",
    "]\n",
    "\n",
    "print(f'源语料文件数：{len(INPUT_FILES):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计输入文件总行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "total_lines = text_files_line_count(INPUT_FILES)\n",
    "print(f'源语料行数：{total_lines:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理\n",
    "\n",
    "我们目前的评估目标\n",
    "\n",
    "1. 进行人工评估，输出列表文件进行比对\n",
    "1. 输入文字为回答数据的逗号前的半句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数定义\n",
    "\n",
    "从段落中拆出开头的几句作为预测上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_SPLIT_PARAGRAPH = re.compile(\n",
    "    r'(?<=[,:,])\\s*'\n",
    ")\n",
    "\n",
    "def split_text(s):\n",
    "    return re.split(RE_SPLIT_PARAGRAPH, s)\n",
    "\n",
    "\n",
    "def proc_line(line):\n",
    "    result = []\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return result\n",
    "    min_length = MIN_LEN\n",
    "    max_length = MAX_LEN\n",
    "    paragraphs = json.loads(line)\n",
    "    for sentences in paragraphs:\n",
    "        paragraph_text = ''.join(s.strip() for s in sentences)\n",
    "        paragraph_text = fix_text(paragraph_text).strip()\n",
    "        parts = split_text(paragraph_text)\n",
    "        population = []\n",
    "        weights = []\n",
    "        for i, v in enumerate(accumulate(len(SP.encode_as_ids(s)) for s in parts[:-1])):\n",
    "            if min_length < v <= max_length:\n",
    "                population.append(i)\n",
    "                weights.append(math.log2(v))\n",
    "        if population:\n",
    "            elems = random.choices(population, weights=weights)\n",
    "            i = elems[0]\n",
    "            d = {\n",
    "                'text': ''.join(parts[:i+1]),\n",
    "                'post_text': ''.join(parts[i+1:]),\n",
    "            }\n",
    "            result.append(d)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文件 tsv/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = '../data/eval.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行\n",
    "\n",
    "并发执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_total_samples = 0\n",
    "\n",
    "with Pool() as pool, open(OUTPUT_FILE, 'w') as fp:\n",
    "    lines_iterator = text_files_line_iterator(INPUT_FILES)\n",
    "    it = pool.imap_unordered(\n",
    "        proc_line,\n",
    "        tqdm(lines_iterator, '[map lines]', total=total_lines),\n",
    "        chunksize=512\n",
    "    )\n",
    "    for result in tqdm(it, '[reduce all]', total=total_lines):\n",
    "        for d in result:\n",
    "            print(json.dumps(d, ensure_ascii=False), file=fp)\n",
    "            n_total_samples += 1\n",
    "\n",
    "print(f'得到语料样本数：{n_total_samples:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l {OUTPUT_FILE} && \\\n",
    "  du -h {OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样\n",
    "\n",
    "我们也许不需要这么多样本进行 evaluation，所以，进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILE) as fp:\n",
    "    n_total_samples = sum(1 for _ in tqdm(fp))\n",
    "\n",
    "print(f'样本数：{n_total_samples:,d}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定需要的采样数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "print(f'n_sample={n_samples:,d} sample_rate={n_samples/n_total_samples*100:.3}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算随机采样 mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lines_mask = np.zeros(n_total_samples, dtype=bool)\n",
    "lines_mask[:n_samples] = True\n",
    "np.random.shuffle(lines_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采样到新的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lines_mask.shape[0]==n_total_samples\n",
    "\n",
    "root, ext = os.path.splitext(OUTPUT_FILE)\n",
    "SPL_FILE = root + f'-spl_{n_sample}' + ext\n",
    "\n",
    "with open(OUTPUT_FILE) as fp_src, open(SPL_FILE, 'w') as fp_dst:\n",
    "    for s in tqdm(compress(fp_src, lines_mask), 'sampling', total=n_sample):\n",
    "        print(s.strip(), file=fp_dst)\n",
    "\n",
    "del lines_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l {SPL_FILE} && \\\n",
    "  du -h {SPL_FILE}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
