{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子 - 使用 `huggingface/transfomers.GPT2LMHeadModel` 进行预测\n",
    "\n",
    "> ❗ **要点**:\n",
    ">\n",
    "> NVIDIA/Megatron-LM 的 `tokenizer` 会在 `128` 对齐的基础上，强制加上 `8` 个特殊 `token`，所以注意 `token` 的偏移！\n",
    "\n",
    "\n",
    "该 Notebook 的代码大部分来自: <https://github.com/huggingface/transformers/blob/master/examples/run_generation.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测时，不需要 `CUDA` 设备:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入模组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from contextlib import closing, ExitStack\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "import sentencepiece as spm\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常量定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最大输出长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT2 模型所在目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIRS = [\n",
    "    'checkpoints/tr_191023',\n",
    "#     'checkpoints/hfgpt2-117m-emotion',\n",
    "#     'checkpoints/gpt2.hf-117m.finetune-baike-dev/checkpoint-100',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentencePiece 模型文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPM_MODEL = 'data/spm/gpt2_huamei_corpus_bpe_32k_v2.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(args):\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def gen_next_token(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0, device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # reptition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in set(generated.view(-1).tolist()):\n",
    "                next_token_logits[i] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: #greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits).unsqueeze(0)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "            yield next_token[0]\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, ids, length=256, printing=True, tqdm_callable=None):\n",
    "    result = ''\n",
    "    stop_flags = [\n",
    "        tokenizer.get_command(n).Id\n",
    "        for n in ('eos', 'pad')\n",
    "    ]\n",
    "    with closing(\n",
    "        gen_next_token(model=model, length=length, context=ids)\n",
    "    ) as iterable:\n",
    "        if tqdm_callable:\n",
    "           iterable =  tqdm_callable(iterable)\n",
    "        for token in iterable:\n",
    "            output_id = int(token.numpy())\n",
    "            if output_id >= 0:\n",
    "                if output_id in stop_flags:\n",
    "                    break\n",
    "                s = tokenizer.DecodeIds([output_id])\n",
    "                result += s\n",
    "                if printing:\n",
    "                    print(s, end='')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentencePiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = make_tokenizer(SentencePieceTokenizer, None, model_path=SPM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "models = {}\n",
    "for model_dir in MODEL_DIRS:\n",
    "    print(f'Load model from {model_dir} ...')   \n",
    "    config = GPT2Config.from_pretrained(model_dir)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir, config=config).eval()\n",
    "    models[model_dir] = model\n",
    "    print('Ok')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测多个短文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 很多人都在说爱情不需要物质，尤其是陷入爱情的女性，往往容易将爱情和物质对立\n",
    "#\n",
    "# 日常生活中经常会遇到一些人，说话很直，经常得罪人，但是他们往往自己并不知道，或者说即便知道也好像不太在乎\n",
    "\n",
    "# 南京长江大桥是长江上的一座桥梁\n",
    "# 我最近迷恋上了早白垩纪土伦阶恐龙演化的相关知识，整天想得都是兽脚类，鸟臀类什么的，是不是心里有问题？很幼稚？能说说你对这个地质年代的知识吗？\n",
    "# 广府文化是广府民系的文化。是以广州为核心、以珠江三角洲为通行范围的粤语文化，它从属于岭南文化，在岭南文化中个性最鲜明、影响最大，在各个领域常被作为粤文化的代称。\n",
    "input_texts = [\n",
    "    '日常生活中经常会遇到一些人，说话很直，经常得罪人',\n",
    "    '南京市长江大桥位于江苏',\n",
    "    '广府文化是广府民系的文化。是以广州为核心、以珠江三角洲为通行范围的粤语文化',\n",
    "    '很多人都在说爱情不需要物质，尤其是陷入爱情的女性',    \n",
    "]\n",
    "\n",
    "input_list = [\n",
    "    [int(n) for n in tokenizer.EncodeAsIds(s.strip())]\n",
    "    for s in input_texts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir, model in models.items():\n",
    "    print(f'Predict with {model_dir} ')\n",
    "    print()\n",
    "\n",
    "    for txt, ids in zip(input_texts, input_list):\n",
    "        print(txt)\n",
    "        \n",
    "        predict(model, tokenizer, ids, length=MAX_OUTPUT_LENGTH)\n",
    "        print()\n",
    "        print('-'*100)\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    print('='*100)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file = 'data/test_xinli_qax_convai.json'\n",
    "output_file = 'data/test_xinli_qax_convai_预测结果-1024_04.tsv'\n",
    "\n",
    "n_gen = 3\n",
    "length = MAX_OUTPUT_LENGTH\n",
    "\n",
    "total = sum(1 for _ in open(input_file))\n",
    "tqdm_callable = lambda x: tqdm(x, desc=f'sample {j+1} ({k+1}/{n_gen})', leave=False)\n",
    "for i, (model_dir, model) in enumerate(models.items()):\n",
    "    print(f'Predict with {model_dir} ')\n",
    "    print()\n",
    "    \n",
    "    root, ext = os.path.splitext(output_file)\n",
    "    output_file = f'{root}.{i+1}{ext}'\n",
    "\n",
    "    with ExitStack() as stack:\n",
    "        fp_input = stack.enter_context(open(input_file))\n",
    "        fp_output = stack.enter_context(open(output_file, 'w'))\n",
    "        writer = csv.writer(fp_output, delimiter='\\t')\n",
    "        \n",
    "        for j, line in tqdm(enumerate(fp_input), total=total):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "            text = (data.get('text') or '').strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            ids = [int(n) for n in tokenizer.EncodeAsIds(text.strip())]\n",
    "            # 可能超长的不要\n",
    "            if len(ids) + length > model.config.n_ctx:\n",
    "                break\n",
    "            row = [text]\n",
    "            for k in range(n_gen):\n",
    "                s_gen = predict(model, tokenizer, ids, length, printing=False, tqdm_callable=tqdm_callable)\n",
    "                row.append(s_gen)\n",
    "            writer.writerow(row)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
