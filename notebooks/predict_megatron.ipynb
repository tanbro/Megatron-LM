{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron 进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD\n",
    "\n",
    "定位到工作目录，根据具体情况决定哦，不一定是下面的命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mpu\n",
    "from data_utils.tokenization import SentencePieceTokenizer, make_tokenizer\n",
    "from pretrain_gpt2 import get_masks_and_position_ids\n",
    "from predict_gpt2 import initialize_distributed, prepare_tokenizer, set_random_seed, setup_model, get_token_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Model arguments\n",
    "    num_layers=12,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=1024,\n",
    "    vocab_size=None,\n",
    "    make_vocab_size_divisible_by=128,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    # Train/valid/test data arguments.\n",
    "    seq_length=512,\n",
    "    model_parallel_size=1,\n",
    "    tokenizer_model_type='bert-large-uncased',\n",
    "    tokenizer_type=SentencePieceTokenizer,\n",
    "    tokenizer_path=\"./data/spm/gpt2_huamei_corpus_bpe_32k_v2.model\",\n",
    "    cache_dir=None,\n",
    "    # Training arguments.\n",
    "    load='./checkpoints/gpt2-117m-xinli001_qa.finetune/',\n",
    "    seed=1234,\n",
    "    checkpoint_activations=None,\n",
    "    checkpoint_num_layers=1,\n",
    "    finetune=None,\n",
    "    no_load_optim=None,\n",
    "    no_load_rng=None,\n",
    "    resume_dataloader=None,\n",
    "    fp16=True,\n",
    "    hysteresis=2,\n",
    "    loss_scale=None,\n",
    "    loss_scale_window=1000,\n",
    "    min_scale=1,\n",
    "    distributed_backend='nccl',\n",
    "    DDP_impl='local',\n",
    "    local_rank=None,\n",
    "    reset_position_ids=None,\n",
    "    reset_attention_mask=None,\n",
    "    eod_mask_loss=None, \n",
    "    # Text generate arguments.\n",
    "    recompute=None,\n",
    "    greedy=False,\n",
    "    top_p=0.0,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    "    out_seq_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1 and model-parallel size: 1 \n",
      " > using dynamic loss scaling\n"
     ]
    }
   ],
   "source": [
    "args.cuda = torch.cuda.is_available()\n",
    "args.rank = int(os.getenv('RANK', '0'))\n",
    "args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
    "\n",
    "if os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n",
    "    # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n",
    "    local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n",
    "    local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n",
    "\n",
    "    # Possibly running with Slurm\n",
    "    num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n",
    "    nodeid = int(os.getenv('SLURM_NODEID', '0'))\n",
    "\n",
    "    args.local_rank = local_rank\n",
    "    args.rank = nodeid*local_size + local_rank\n",
    "    args.world_size = num_nodes*local_size\n",
    "\n",
    "args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n",
    "if args.rank == 0:\n",
    "    print('using world size: {} and model-parallel size: {} '.format(\n",
    "        args.world_size, args.model_parallel_size))\n",
    "\n",
    "args.dynamic_loss_scale = False\n",
    "if args.loss_scale is None:\n",
    "    args.dynamic_loss_scale = True\n",
    "    if args.rank == 0:\n",
    "        print(' > using dynamic loss scaling')\n",
    "\n",
    "# The args fp32_* or fp16_* meant to be active when the\n",
    "# args fp16 is set. So the default behavior should all\n",
    "# be false.\n",
    "if not args.fp16:\n",
    "    args.fp32_embedding = False\n",
    "    args.fp32_tokentypes = False\n",
    "    args.fp32_layernorm = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "prepare tokenizer done\n",
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 0: 110516736\n",
      "global rank 0 is loading checkpoint ./checkpoints/gpt2-117m-xinli001_qa.finetune/iter_0035000/mp_rank_00/model_optim_rng.pt\n",
      "  successfully loaded ./checkpoints/gpt2-117m-xinli001_qa.finetune/iter_0035000/mp_rank_00/model_optim_rng.pt\n",
      "CPU times: user 5.13 s, sys: 1.73 s, total: 6.87 s\n",
      "Wall time: 6.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Disable CuDNN.\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# Pytorch distributed.\n",
    "initialize_distributed(args)\n",
    "\n",
    "# Random seeds for reproducability.\n",
    "set_random_seed(args.seed)\n",
    "\n",
    "# get the tokenizer\n",
    "tokenizer = prepare_tokenizer(args)\n",
    "\n",
    "# Model, optimizer, and learning rate.\n",
    "model = setup_model(args)\n",
    "\n",
    "args.device = torch.cuda.current_device()\n",
    "\n",
    "# setting default batch size to 1\n",
    "args.batch_size = 1\n",
    "\n",
    "assert mpu.get_model_parallel_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_generative(contex_text, model, tokenizer):\n",
    "    contex_text = contex_text.strip()\n",
    "    context_tokens = tokenizer.EncodeAsIds(contex_text).tokenization\n",
    "    context_length = len(context_tokens)\n",
    "\n",
    "    if context_length >= args.seq_length//2:\n",
    "        print(\n",
    "            f\"\\nContext length {context_length}\"\n",
    "            \"\\nPlease give smaller context (half of the sequence length)!\",\n",
    "            file=sys.stderr\n",
    "        )\n",
    "\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "    for output_tokens, _ in token_stream:\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        s = tokenizer.DecodeIds([ids[-1]])\n",
    "        yield s\n",
    "\n",
    "def infer_next_token_generative(context_tokens, model):\n",
    "    context_length = len(context_tokens)\n",
    "\n",
    "    if context_length >= args.seq_length//2:\n",
    "        print(\n",
    "            f\"\\nContext length {context_length}\"\n",
    "            \"\\nPlease give smaller context (half of the sequence length)!\",\n",
    "            file=sys.stderr\n",
    "        )\n",
    "\n",
    "    token_stream = get_token_stream(model, [context_tokens], tokenizer, args)\n",
    "    for output_tokens, _ in token_stream:\n",
    "        ids = output_tokens.cpu().numpy().tolist()[0]\n",
    "        next_token = ids[-1]\n",
    "        yield next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    (\n",
    "        '最近晚上睡不着觉，但很困，总觉得不会睡觉？',\n",
    "        '最近一段时间，总是晚上睡不着觉，但是却很困，没次睡觉心理都在不自觉的想着该如何去睡觉，每次想着想着就完全清醒了，失眠特别严重，感觉身体快要透支了，很无助，还连累家人一起跟着担心'\n",
    "    ),\n",
    "    (\n",
    "        '大一，谈了将近一年的女友突然提出分手我该怎么办？',\n",
    "        ' 某一天晚上她突然给我打电话说我不是她喜欢的那种类型，在电话中她多次提到自己的性格不好，说“我所看到的都是加了“滤镜”的效果，其实她并没有我想得那么好，今后不准备结婚”。（她以前谈过一次恋爱，对于那场恋爱她一直都铭记在心）接完电话我很迷茫和困惑，因为之前她愿意跟我分享她的生活中的不如意和困难，突然的转变让我措手不及。同时我也很自责，没有准确的表达出自己的意思和情感，在与她的对话中我常常感到无力和无助。我到底该怎么办？我真的很爱她，想和她度过一生。'\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最近晚上睡不着觉，但很困，总觉得不会睡觉？\n",
      "最近一段时间，总是晚上睡不着觉，但是却很困，没次睡觉心理都在不自觉的想着该如何去睡觉，每次想着想着就完全清醒了，失眠特别严重，感觉身体快要透支了，很无助，还连累家人一起跟着担心\n",
      "\n",
      "0): 焦虑的原因已经有大概,焦虑症有情绪低落,却没有自残的行为,焦虑,过度关注自身的原因,化跑线比较差,并且过度关注自身的变化,是想发生什么事情来进展。如果出现泛化,是不是惊恐发作,导致焦虑、抑郁。 <eos>\n",
      "\n",
      "1): 焦虑情绪息都会让我们措手不及,从而影响你的学习,生活,可以暂时自己调节 <eos>\n",
      "\n",
      "2): 药物治疗,只有两条路才能碰到你是母亲的症状。但是不知道你指向谁吐的行为呢?看起来你有些社交焦虑,有人可以帮你安心的缓解。一步步入正轨上,跑步是跑步的,一方面是很多人都会有挑战,但一方面了解客观条件,之后就可以决定好了。 <eos>\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "大一，谈了将近一年的女友突然提出分手我该怎么办？\n",
      " 某一天晚上她突然给我打电话说我不是她喜欢的那种类型，在电话中她多次提到自己的性格不好，说“我所看到的都是加了“滤镜”的效果，其实她并没有我想得那么好，今后不准备结婚”。（她以前谈过一次恋爱，对于那场恋爱她一直都铭记在心）接完电话我很迷茫和困惑，因为之前她愿意跟我分享她的生活中的不如意和困难，突然的转变让我措手不及。同时我也很自责，没有准确的表达出自己的意思和情感，在与她的对话中我常常感到无力和无助。我到底该怎么办？我真的很爱她，想和她度过一生。\n",
      "\n",
      "0): 你好!困扰是时候思想和想法在作祟,看到你就不再神秘了!当你认为两人互相吸引的时候,你不敢来主动去动挡他,敢继续做朋友的关心,让自己远一些,这样让我不知道要如何去爱她。 <eos>\n",
      "\n",
      "1): 呵呵,看完你的文字我感受到你是一个很空洞的人,面对这样的情况,有一段不太容易渡过这段伤痛。回忆是这段经历中回忆和过去经历的体验,似乎成了你成长中的一段经历,你对前任的态度和前任的态度,似乎在情感上遇到了了一些疑惑。接纳自己,才是真正的前任由过去的去面对和处理。 <eos>\n",
      "\n",
      "2): 看完这个问题,看的出来,没什么大不了的,性格决定中,适应新的环境,是要适应一下就改变一下,如果找不到原因,可以选择咨询。 <eos>\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gen = 3\n",
    "\n",
    "for title, text in input_texts:\n",
    "    txt = f'{title}<sep>{text}<|endoftext|>'\n",
    "    context_tokens = tokenizer.EncodeAsIds(txt).tokenization\n",
    "    print(title)\n",
    "    print(text)\n",
    "    print()\n",
    "    for i in range(n_gen):\n",
    "        s_pred = ''\n",
    "        for s in infer_generative(txt, model, tokenizer):\n",
    "            if '<|endoftext|>' in s:\n",
    "                s_pred = ''\n",
    "            else:\n",
    "                s_pred += s\n",
    "        print(f'{i}): {s_pred}')\n",
    "        print()\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Megatron_LM-ipy]",
   "language": "python",
   "name": "conda-env-Megatron_LM-ipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
